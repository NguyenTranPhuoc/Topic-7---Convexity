{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY6vWIj8cVBR"
      },
      "source": [
        "# Team Member"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1KAitW-cVBT"
      },
      "source": [
        "Nguyễn Minh Tuấn - 2470574\n",
        "\n",
        "Nguyễn Trần Phước - 2470576\n",
        "\n",
        "Lê Trí Quyền - 2470740\n",
        "\n",
        "Lê Quang Trung - 2470746\n",
        "\n",
        "Trần Hoàng Nguyên - 2470739"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fhUeDpNcVBT"
      },
      "source": [
        "# Topic 7: Convexity (Section 12.1-12.2): https://d2l.ai/chapter_optimization/index.html  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgAz4PgDcVBT"
      },
      "source": [
        "# 12.1 Optimization and Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgiTi3PRcVBT"
      },
      "source": [
        "## Introduction\n",
        "Loss function is a mathematical function that measures how well a machine learning or deep learning model performs on a given task by quantifying the difference between the model's predictions and the true target values.\n",
        "\n",
        "Optimization algorithms are the tools that allow us to continue updating model parameters and to minimize the value of the loss function, as evaluated on the training set.\n",
        "\n",
        "On the one hand, training a complex deep learning model can take hours, days, or even weeks. The performance of the optimization algorithm directly affects the model’s training efficiency. On the other hand, understanding the principles of different optimization algorithms and the role of their hyperparameters will enable us to tune the hyperparameters in a targeted manner to improve the performance of deep learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If5xWXeTcVBU"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install torch\n",
        "!pip install d2l\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwx2XJHDcVBU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krYWlpPecVBV"
      },
      "source": [
        "## 12.1.1 Goal of optimization\n",
        "\n",
        "Training Error (Empirical Error): The error of a model on the same data it was trained on. It measures how well the model fits the training data.\n",
        "\n",
        "Generalization Error (True Error): The expected error of the model on new, unseen data drawn from the same distribution P(x,y). It Measures how well the model generalizes to real-world data.\n",
        "\n",
        "The goal of optimization is to reduce the training error. However, the goal of deep learning (or more broadly, statistical inference) is to reduce the generalization error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXg8TJ77cVBV"
      },
      "source": [
        "![Loss function.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_5djCU_cVBV"
      },
      "source": [
        "### Example:\n",
        "empirical risk and true risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6XXG1iucVBW"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate true distribution (y = 2x + 1 + noise)\n",
        "X_true = np.linspace(0, 5, 100)\n",
        "y_true = 2 * X_true + 1 + np.random.normal(scale=1, size=100)\n",
        "\n",
        "# Small empirical dataset (20 samples)\n",
        "X_empirical = np.random.uniform(0, 5, 20)\n",
        "y_empirical = 2 * X_empirical + 1 + np.random.normal(scale=1, size=20)\n",
        "\n",
        "# Fit linear regression to empirical data\n",
        "model = LinearRegression()\n",
        "model.fit(X_empirical.reshape(-1, 1), y_empirical)\n",
        "\n",
        "# Predictions\n",
        "y_pred_empirical = model.predict(X_empirical.reshape(-1, 1))\n",
        "y_pred_true = model.predict(X_true.reshape(-1, 1))\n",
        "\n",
        "# Calculate risks\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "empirical_risk = mse(y_empirical, y_pred_empirical)\n",
        "true_risk = mse(y_true, y_pred_true)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# True distribution\n",
        "plt.scatter(X_true, y_true, alpha=0.3, label=\"True distribution (P)\", color='blue')\n",
        "plt.plot(X_true, 2*X_true+1, '--', label=\"True function (f*)\", color='black')\n",
        "\n",
        "# Empirical data & model\n",
        "plt.scatter(X_empirical, y_empirical, label=\"Empirical data\", color='red', zorder=10)\n",
        "plt.plot(X_true, y_pred_true, label=f\"Empirical model (R_emp={empirical_risk:.2f}, R_true={true_risk:.2f})\", color='red')\n",
        "\n",
        "plt.title(\"Empirical Risk vs. True Risk\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsgMszAucVBW"
      },
      "source": [
        "## 12.1.2 Optimization Challenges in Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHabUQeFcVBW"
      },
      "source": [
        "### 12.1.2.1 Local Minima\n",
        "\n",
        "For any objective function f(x), if the value of f(x) at m is smaller than the values of f(x) at any other points in the vicinity of m, then f(m) could be a local minimum. If the value of f(x) at m is the minimum of the objective function over the entire domain, then f(m) is the global minimum.\n",
        "\n",
        "The objective function of deep learning models usually has many local optima.  When the numerical solution of an optimization problem is near the local optimum, the numerical solution may only minimize the objective function locally, rather than globally, as the gradient of the objective function’s solutions approaches or becomes zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAiIfUHdcVBW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import torch\n",
        "from mpl_toolkits import mplot3d\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lokBLQhAcVBW"
      },
      "source": [
        "#### Example.\n",
        "Local Minimal: function: f(x) = x.sin(πx) with x in [ - 1 , 2 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUdLU7dgcVBW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the function\n",
        "def f(x):\n",
        "    return x * np.sin(np.pi * x)\n",
        "\n",
        "# Generate data points\n",
        "x = np.linspace(-1, 2, 500)\n",
        "y = f(x)\n",
        "\n",
        "# Find roots and extrema\n",
        "roots = x[np.abs(y) < 1e-8]  # Approximate roots\n",
        "critical_points = x[1:-1][np.diff(np.sign(np.diff(y))) != 0]  # Extrema via derivative sign change\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y, label=r'$f(x) = x \\sin(\\pi x)$', color='blue', linewidth=2)\n",
        "\n",
        "# Highlight key features\n",
        "plt.scatter(roots, f(roots), color='red', zorder=5, label='Roots (f(x)=0)')\n",
        "plt.scatter(critical_points, f(critical_points), color='green', zorder=5, label='Local Extrema')\n",
        "\n",
        "# Annotate special points\n",
        "for xp in roots:\n",
        "    plt.annotate(f'({xp:.1f}, 0)', (xp, 0), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "for xp in critical_points:\n",
        "    plt.annotate(f'({xp:.2f}, {f(xp):.2f})',\n",
        "                (xp, f(xp)),\n",
        "                textcoords=\"offset points\",\n",
        "                xytext=(10,-15), ha='left')\n",
        "\n",
        "# Add reference lines\n",
        "plt.axhline(0, color='black', linestyle=':', alpha=0.5)\n",
        "plt.axvline(0, color='black', linestyle=':', alpha=0.5)\n",
        "plt.axvline(1, color='purple', linestyle='--', alpha=0.5, label='x=1 (Special point)')\n",
        "\n",
        "plt.title(r'Behavior of $f(x) = x \\sin(\\pi x)$ on $[-1, 2]$', pad=20)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMEjAiiIcVBW"
      },
      "source": [
        "### 12.1.2.2. Saddle Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiQek1ficVBW"
      },
      "source": [
        "#### Import library for code example in this section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG3TwF9zcVBW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from mpl_toolkits import mplot3d\n",
        "from d2l import torch as d2l\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjY6TETLcVBW"
      },
      "outputs": [],
      "source": [
        "def annotate(text, xy, xytext):\n",
        "    d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,\n",
        "                           arrowprops=dict(arrowstyle='->'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SixF5oMNcVBW"
      },
      "source": [
        "#### 1. Introduction\n",
        "##### 1.1. Problem Statement\n",
        "In optimization problems, especially in deep learning, the goal is to find parameter values for an objective function (typically a loss function) that minimize it. Gradient-based optimization algorithms (e.g., Gradient Descent) navigate the function landscape by moving in the direction opposite to the gradient. A critical point is where the gradient is zero.\n",
        "\n",
        "A **saddle point** is a critical point that is neither a local minimum nor a local maximum. At a saddle point, the function increases along some dimensions and decreases along others, resembling a horse's saddle.\n",
        "\n",
        "**The Challenge:** Saddle points are problematic for first-order optimization algorithms. Since the gradient is zero, algorithms might converge slowly or get stuck, mistaking the saddle point for a minimum. In high-dimensional spaces common in deep learning, saddle points are often more prevalent than local minima.\n",
        "\n",
        "##### 1.2. Applications\n",
        "Understanding and addressing saddle points is crucial in:\n",
        "* **Deep Learning:** Training neural networks involves optimizing high-dimensional, non-convex loss functions rife with saddle points.\n",
        "* **Signal Processing:** Problems like signal recovery.\n",
        "* **Game Theory:** Finding equilibria.\n",
        "* **Optimal Control:** Designing efficient controllers.\n",
        "\n",
        "##### 1.3. Addressing Saddle Points in Computer Science\n",
        "The study of saddle points has led to more robust optimization methods:\n",
        "* **Second-order methods:** (e.g., Newton's method) use Hessian information to distinguish saddle points from minima. Costly for large models.\n",
        "* **Adaptive learning rate algorithms:** (e.g., AdaGrad, RMSProp, Adam) adjust learning rates per parameter, aiding navigation of complex landscapes.\n",
        "* **Stochasticity and Noise:** Stochastic Gradient Descent (SGD) with mini-batches inherently introduces noise, which can help escape saddle points. Perturbed gradient descent methods add noise intentionally.\n",
        "* **Exploiting Negative Curvature:** Algorithms that detect and follow directions of negative curvature (if available at a saddle point) to continue descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM1JLS6pcVBW"
      },
      "source": [
        "#### 2. Detailed Computations with Concrete Examples\n",
        "\n",
        "To classify a critical point (where gradient $\\nabla f = 0$), we use the Hessian matrix $H$ and its eigenvalues ($\\lambda$). The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. It describes the local curvature of the function.\n",
        "\n",
        "\n",
        "##### How to Compute the Hessian Matrix\n",
        "For a function of two variables, $f(x, y)$, the Hessian matrix is defined as:\n",
        "$$H(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix}$$\n",
        "\n",
        "Let's break down each component:\n",
        "\n",
        "1.  **$\\frac{\\partial^2 f}{\\partial x^2}$:**\n",
        "    * First, find the partial derivative of $f$ with respect to $x$, treating $y$ as a constant: $\\frac{\\partial f}{\\partial x}$.\n",
        "    * Then, differentiate this result again with respect to $x$, still treating $y$ as a constant.\n",
        "\n",
        "2.  **$\\frac{\\partial^2 f}{\\partial y^2}$:**\n",
        "    * First, find the partial derivative of $f$ with respect to $y$, treating $x$ as a constant: $\\frac{\\partial f}{\\partial y}$.\n",
        "    * Then, differentiate this result again with respect to $y$, still treating $x$ as a constant.\n",
        "\n",
        "3.  **$\\frac{\\partial^2 f}{\\partial x \\partial y}$:** These are mixed partial derivatives.\n",
        "    * For $\\frac{\\partial^2 f}{\\partial x \\partial y}$: First, find the partial derivative of $f$ with respect to $y$ ($\\frac{\\partial f}{\\partial y}$). Then, differentiate that result with respect to $x$.\n",
        "    * For $\\frac{\\partial^2 f}{\\partial y \\partial x}$: First, find the partial derivative of $f$ with respect to $x$ ($\\frac{\\partial f}{\\partial x}$). Then, differentiate that result with respect to $y$.\n",
        "\n",
        "**Clairaut's Theorem (Symmetry of Second Derivatives):** For most well-behaved functions (specifically, if the second partial derivatives are continuous), the order of differentiation in mixed partials does not matter. This means:\n",
        "$$\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x}$$\n",
        "Because of this symmetry, the Hessian matrix is a symmetric matrix.\n",
        "\n",
        "##### How to Calculate Eigenvalues\n",
        "\n",
        "Once we have the Hessian matrix $H$ (or any square matrix), its eigenvalues ($\\lambda$) can be found by solving the characteristic equation. Here's the process:\n",
        "\n",
        "1.  **Start with the Eigenvalue Equation:**\n",
        "    The fundamental equation defining an eigenvalue $\\lambda$ and its corresponding non-zero eigenvector $\\mathbf{v}$ for a matrix $H$ is:\n",
        "    $$H\\mathbf{v} = \\lambda\\mathbf{v}$$\n",
        "\n",
        "2.  **Rearrange the Equation:**\n",
        "    To solve for $\\lambda$, we rearrange the equation:\n",
        "    $$H\\mathbf{v} - \\lambda\\mathbf{v} = \\mathbf{0}$$\n",
        "    Introduce the identity matrix $I$ (of the same dimensions as $H$) to factor out $\\mathbf{v}$:\n",
        "    $$H\\mathbf{v} - \\lambda I\\mathbf{v} = \\mathbf{0}$$\n",
        "    $$(H - \\lambda I)\\mathbf{v} = \\mathbf{0}$$\n",
        "\n",
        "3.  **Condition for Non-Trivial Solutions:**\n",
        "    This is a homogeneous system of linear equations. For a non-zero eigenvector $\\mathbf{v}$ to exist (which is required by definition), the matrix $(H - \\lambda I)$ must be singular. A singular matrix is one whose determinant is zero.\n",
        "\n",
        "4.  **Form the Characteristic Equation:**\n",
        "    Therefore, we set the determinant of $(H - \\lambda I)$ to zero:\n",
        "    $$\\det(H - \\lambda I) = 0$$\n",
        "    This equation is called the **characteristic equation** of matrix $H$. It's a polynomial equation in $\\lambda$. The degree of the polynomial is equal to the dimension of the matrix $H$.\n",
        "\n",
        "5.  **Solve for Eigenvalues:**\n",
        "    The roots of the characteristic equation are the eigenvalues of the matrix $H$.\n",
        "\n",
        "**Classification using Eigenvalues of the Hessian at a critical point ($\\nabla f = 0$):**\n",
        "* All $\\lambda_k > 0$: Local minimum.\n",
        "* All $\\lambda_k < 0$: Local maximum.\n",
        "* Mixed positive and negative $\\lambda_k$: Saddle point.\n",
        "* Some $\\lambda_k = 0$: Inconclusive (requires further analysis).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QsoafZ0cVBX"
      },
      "source": [
        "##### Example 1 (1D): $f(x) = x^3$\n",
        "This function has a critical point at $x=0$. The first derivative $f'(x) = 3x^2$ is 0 at $x=0$. The second derivative $f''(x) = 6x$ is also 0 at $x=0$. While the second derivative test is inconclusive here for 1D min/max, $x=0$ is an inflection point with a horizontal tangent. It illustrates a scenario where the gradient is zero, but it's not a local extremum, analogous to higher-dimensional saddle points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7urtXy-TcVBX"
      },
      "outputs": [],
      "source": [
        "x = torch.arange(-2.0, 2.0, 0.01)\n",
        "d2l.plot(x, [x**3], 'x', 'f(x)')\n",
        "annotate('saddle point', (0, -0.2), (-0.52, -5.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KG2wGd_cVBX"
      },
      "source": [
        "##### Example 2 (2D): $f(x, y) = x^2 - y^2$ (Classic Saddle Point)\n",
        "\n",
        "This example demonstrates a classic saddle point.\n",
        "\n",
        "**1. Find Critical Points:**\n",
        "The first step is to find where the gradient $\\nabla f(x,y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)$ is zero.\n",
        "* $\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x}(x^2 - y^2) = 2x$\n",
        "* $\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y}(x^2 - y^2) = -2y$\n",
        "\n",
        "Set the gradient components to zero:\n",
        "* $2x = 0 \\implies x = 0$\n",
        "* $-2y = 0 \\implies y = 0$\n",
        "So, the only critical point is $(0,0)$.\n",
        "\n",
        "**2. The Hessian Matrix $H(x,y)$:**\n",
        "The Hessian matrix is $H(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix}$.\n",
        "* **$\\frac{\\partial^2 f}{\\partial x^2}$**: Differentiate $\\frac{\\partial f}{\\partial x} = 2x$ with respect to $x$:\n",
        "    $$\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x}(2x) = 2$$\n",
        "* **$\\frac{\\partial^2 f}{\\partial y^2}$**: Differentiate $\\frac{\\partial f}{\\partial y} = -2y$ with respect to $y$:\n",
        "    $$\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y}(-2y) = -2$$\n",
        "* **$\\frac{\\partial^2 f}{\\partial x \\partial y}$**: Differentiate $\\frac{\\partial f}{\\partial y} = -2y$ with respect to $x$:\n",
        "    $$\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial x}(-2y) = 0$$\n",
        "* **$\\frac{\\partial^2 f}{\\partial y \\partial x}$**: Differentiate $\\frac{\\partial f}{\\partial x} = 2x$ with respect to $y$:\n",
        "    $$\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial}{\\partial y}(2x) = 0$$\n",
        "So, the Hessian matrix (which is constant for this function) is:\n",
        "$$H(x,y) = H(0,0) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}$$\n",
        "\n",
        "**3. Eigenvalues for $H = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}$:**\n",
        "We solve $\\det(H - \\lambda I) = 0$.\n",
        "* $H - \\lambda I = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2-\\lambda & 0 \\\\ 0 & -2-\\lambda \\end{pmatrix}$\n",
        "* $\\det(H - \\lambda I) = (2-\\lambda)(-2-\\lambda) - (0)(0) = (2-\\lambda)(-2-\\lambda)$\n",
        "* Set the determinant to zero: $(2-\\lambda)(-2-\\lambda) = 0$\n",
        "* This gives the eigenvalues:\n",
        "    * $2-\\lambda = 0 \\implies \\lambda_1 = 2$\n",
        "    * $-2-\\lambda = 0 \\implies \\lambda_2 = -2$\n",
        "\n",
        "**4. Classification of the Critical Point $(0,0)$:**\n",
        "Since the eigenvalues are $\\lambda_1 = 2$ (positive) and $\\lambda_2 = -2$ (negative), the critical point $(0,0)$ is a **saddle point**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0Pu5xpdcVBX"
      },
      "outputs": [],
      "source": [
        "# Example 2: f(x,y) = x^2 - y^2\n",
        "def f_example2(x, y):\n",
        "    return x**2 - y**2\n",
        "\n",
        "def hessian_f_example2(x, y): # Constant for this function\n",
        "    return np.array([[2, 0], [0, -2]])\n",
        "\n",
        "x_c2, y_c2 = 0, 0\n",
        "H2 = hessian_f_example2(x_c2, y_c2)\n",
        "eigenvalues2 = np.linalg.eigvals(H2)\n",
        "\n",
        "print(f\"--- Analysis of Example 2: f(x, y) = x^2 - y^2 ---\")\n",
        "print(f\"Critical Point: ({x_c2}, {y_c2})\")\n",
        "print(f\"  f({x_c2}, {y_c2}) = {f_example2(x_c2, y_c2):.4f}\")\n",
        "print(f\"  Hessian matrix at ({x_c2}, {y_c2}):\\n{H2}\")\n",
        "print(f\"  Eigenvalues of Hessian: {eigenvalues2}\")\n",
        "if np.any(eigenvalues2 > 1e-9) and np.any(eigenvalues2 < -1e-9):\n",
        "    print(f\"  Classification: Saddle Point\")\n",
        "else:\n",
        "    print(f\"  Classification: Check logic or eigenvalues\")\n",
        "\n",
        "# Visualization for Example 2\n",
        "x_range2 = np.linspace(-2, 2, 50)\n",
        "y_range2 = np.linspace(-2, 2, 50)\n",
        "X2, Y2 = np.meshgrid(x_range2, y_range2)\n",
        "Z2 = f_example2(X2, Y2)\n",
        "\n",
        "fig2 = plt.figure(figsize=(10, 7))\n",
        "ax2 = fig2.add_subplot(111, projection='3d')\n",
        "surf2 = ax2.plot_surface(X2, Y2, Z2, cmap='coolwarm', alpha=0.8, edgecolor='k', linewidth=0.1)\n",
        "ax2.scatter(0, 0, 0, color='black', s=150, label='(0,0): Saddle Point', depthshade=True, marker='X')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('y')\n",
        "ax2.set_zlabel('f(x, y)')\n",
        "ax2.set_title('Surface of $f(x, y) = x^2 - y^2$ (Classic Saddle Point)')\n",
        "fig2.colorbar(surf2, shrink=0.5, aspect=5, label='f(x,y)')\n",
        "ax2.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2wbyGVxcVBX"
      },
      "source": [
        "##### Example 3 (2D): $f(x, y) = x^3 - 3x + y^2 + 2y$\n",
        "\n",
        "**1. Find Critical Points:**\n",
        "$\\nabla f = (3x^2 - 3, 2y + 2)$.\n",
        "Set gradient to $(0,0)$:\n",
        "* $3x^2 - 3 = 0 \\implies 3x^2 = 3 \\implies x^2 = 1 \\implies x = 1$ or $x = -1$.\n",
        "* $2y + 2 = 0 \\implies 2y = -2 \\implies y = -1$.\n",
        "The critical points are $(1, -1)$ and $(-1, -1)$.\n",
        "\n",
        "**2. The Hessian Matrix $H(x,y)$:**\n",
        "For $f(x, y) = x^3 - 3x + y^2 + 2y$:\n",
        "* $\\frac{\\partial f}{\\partial x} = 3x^2 - 3$\n",
        "* $\\frac{\\partial f}{\\partial y} = 2y + 2$\n",
        "\n",
        "Now, the second-order partial derivatives:\n",
        "* **$\\frac{\\partial^2 f}{\\partial x^2}$**: $\\frac{\\partial}{\\partial x}(3x^2 - 3) = 6x$\n",
        "* **$\\frac{\\partial^2 f}{\\partial y^2}$**: $\\frac{\\partial}{\\partial y}(2y + 2) = 2$\n",
        "* **$\\frac{\\partial^2 f}{\\partial x \\partial y}$**: $\\frac{\\partial}{\\partial x}(2y + 2) = 0$\n",
        "* **$\\frac{\\partial^2 f}{\\partial y \\partial x}$**: $\\frac{\\partial}{\\partial y}(3x^2 - 3) = 0$\n",
        "\n",
        "So, the Hessian matrix is:\n",
        "$$H(x,y) = \\begin{pmatrix} 6x & 0 \\\\ 0 & 2 \\end{pmatrix}$$\n",
        "\n",
        "**3. Classification of Critical Points :**\n",
        "\n",
        "* **For Critical Point $(1, -1)$:**\n",
        "    * **Hessian at $(1,-1)$:**\n",
        "        $$H(1,-1) = \\begin{pmatrix} 6(1) & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 6 & 0 \\\\ 0 & 2 \\end{pmatrix}$$\n",
        "    * **Eigenvalue Calculation for $H(1,-1)$:**\n",
        "        Solve $\\det(H(1,-1) - \\lambda I) = 0$.\n",
        "        $H(1,-1) - \\lambda I = \\begin{pmatrix} 6-\\lambda & 0 \\\\ 0 & 2-\\lambda \\end{pmatrix}$\n",
        "        $\\det(H(1,-1) - \\lambda I) = (6-\\lambda)(2-\\lambda) - 0 = 0$\n",
        "        Eigenvalues: $\\lambda_1 = 6, \\lambda_2 = 2$.\n",
        "    * **Classification:** Both eigenvalues are positive ($6 > 0, 2 > 0$), so $(1, -1)$ is a **local minimum**.\n",
        "\n",
        "* **For Critical Point $(-1, -1)$:**\n",
        "    * **Hessian at $(-1,-1)$:**\n",
        "        $$H(-1,-1) = \\begin{pmatrix} 6(-1) & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} -6 & 0 \\\\ 0 & 2 \\end{pmatrix}$$\n",
        "    * **Eigenvalue Calculation for $H(-1,-1)$:**\n",
        "        Solve $\\det(H(-1,-1) - \\lambda I) = 0$.\n",
        "        $H(-1,-1) - \\lambda I = \\begin{pmatrix} -6-\\lambda & 0 \\\\ 0 & 2-\\lambda \\end{pmatrix}$\n",
        "        $\\det(H(-1,-1) - \\lambda I) = (-6-\\lambda)(2-\\lambda) - 0 = 0$\n",
        "        Eigenvalues: $\\lambda_1 = -6, \\lambda_2 = 2$.\n",
        "    * **Classification:** Eigenvalues have mixed signs ($-6 < 0, 2 > 0$), so $(-1, -1)$ is a **saddle point**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4kw5qUhcVBX"
      },
      "outputs": [],
      "source": [
        "# Example 3: f(x, y) = x^3 - 3x + y^2 + 2y\n",
        "def f_example3(x, y):\n",
        "    return x**3 - 3*x + y**2 + 2*y\n",
        "\n",
        "def grad_f_example3(x, y): # For verification\n",
        "    df_dx = 3*x**2 - 3\n",
        "    df_dy = 2*y + 2\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "def hessian_f_example3(x, y):\n",
        "    d2f_dx2 = 6*x\n",
        "    d2f_dy2 = 2\n",
        "    d2f_dxdy = 0\n",
        "    return np.array([[d2f_dx2, d2f_dxdy],\n",
        "                     [d2f_dxdy, d2f_dy2]])\n",
        "\n",
        "critical_points_ex3 = [(1, -1), (-1, -1)]\n",
        "labels_ex3 = [\"Local Minimum\", \"Saddle Point\"]\n",
        "colors_ex3 = ['blue', 'red']\n",
        "markers_ex3 = ['o', 'X']\n",
        "\n",
        "\n",
        "print(f\"\\n--- Analysis of Example 3: f(x, y) = x^3 - 3x + y^2 + 2y ---\")\n",
        "for i, (x_c, y_c) in enumerate(critical_points_ex3):\n",
        "    print(f\"\\nCritical Point {i+1}: ({x_c}, {y_c})\")\n",
        "    f_val = f_example3(x_c, y_c)\n",
        "    print(f\"  f({x_c}, {y_c}) = {f_val:.4f}\")\n",
        "    grad_val = grad_f_example3(x_c, y_c) # Should be [0,0]\n",
        "    print(f\"  Gradient at ({x_c}, {y_c}): {grad_val}\")\n",
        "\n",
        "    H = hessian_f_example3(x_c, y_c)\n",
        "    eigenvalues = np.linalg.eigvals(H)\n",
        "    print(f\"  Hessian matrix at ({x_c}, {y_c}):\\n{H}\")\n",
        "    print(f\"  Eigenvalues of Hessian: {eigenvalues}\")\n",
        "\n",
        "    if np.all(eigenvalues > 1e-9):\n",
        "        print(f\"  Classification: Local Minimum (Matches: {labels_ex3[i]})\")\n",
        "    elif np.all(eigenvalues < -1e-9):\n",
        "        print(f\"  Classification: Local Maximum\") # Not expected for this example\n",
        "    elif np.any(eigenvalues > 1e-9) and np.any(eigenvalues < -1e-9):\n",
        "        print(f\"  Classification: Saddle Point (Matches: {labels_ex3[i]})\")\n",
        "    else:\n",
        "        print(f\"  Classification: Inconclusive (Matches: {labels_ex3[i]})\")\n",
        "\n",
        "\n",
        "# Visualization for Example 3\n",
        "x_range3 = np.linspace(-2.5, 2.5, 100)\n",
        "y_range3 = np.linspace(-3, 1, 100)\n",
        "X3, Y3 = np.meshgrid(x_range3, y_range3)\n",
        "Z3 = f_example3(X3, Y3)\n",
        "\n",
        "fig3 = plt.figure(figsize=(12, 8))\n",
        "ax3 = fig3.add_subplot(111, projection='3d')\n",
        "surf3 = ax3.plot_surface(X3, Y3, Z3, cmap='viridis', alpha=0.7, edgecolor='k', linewidth=0.1, rstride=5, cstride=5) # rstride/cstride for performance\n",
        "\n",
        "for i, (x_c, y_c) in enumerate(critical_points_ex3):\n",
        "    f_c = f_example3(x_c, y_c)\n",
        "    ax3.scatter(x_c, y_c, f_c, color=colors_ex3[i], s=150, label=f'({x_c},{y_c}): {labels_ex3[i]}', depthshade=True, marker=markers_ex3[i])\n",
        "\n",
        "ax3.set_xlabel('x')\n",
        "ax3.set_ylabel('y')\n",
        "ax3.set_zlabel('f(x, y)')\n",
        "ax3.set_title('Surface of $f(x,y) = x^3 - 3x + y^2 + 2y$ and Critical Points')\n",
        "fig3.colorbar(surf3, shrink=0.5, aspect=5, label='f(x,y)')\n",
        "\n",
        "# Handle legend to avoid duplicates if any (though direct labeling is fine here)\n",
        "handles, labels_legend = ax3.get_legend_handles_labels()\n",
        "by_label = dict(zip(labels_legend, handles))\n",
        "ax3.legend(by_label.values(), by_label.keys())\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8XYzLH6cVBX"
      },
      "source": [
        "### 12.1.2.3. Vanishing Gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xmi4d0rcVBX"
      },
      "source": [
        "#### 1. Introduction\n",
        "\n",
        "##### 1.1. Problem Statement: What are Vanishing Gradients?\n",
        "The **vanishing gradient problem** is a significant challenge encountered during the training of deep artificial neural networks. It refers to the phenomenon where the gradients of the loss function with respect to the weights in the earlier layers of the network become exceedingly small (i.e., they \"vanish\").\n",
        "\n",
        "When these gradients are very close to zero, the updates to the weights during backpropagation become minuscule. Consequently, the earlier layers of the network learn very slowly, or sometimes not at all. This is problematic because these initial layers are often responsible for learning fundamental, low-level features from the input data, which are crucial for the subsequent layers to build upon.\n",
        "\n",
        "The d2l.ai book (Section 12.1.2.3) highlights this with an example: when minimizing a function like $f(x) = \\tanh(x)$, if the starting point is far from the origin (e.g., $x=4$), the gradient $f'(x)$ is already very small, making optimization progress extremely slow. This effect is compounded in deep networks.\n",
        "\n",
        "##### 1.2. Impact on Deep Learning\n",
        "Historically, the vanishing gradient problem was a major obstacle to training truly deep neural networks. Before effective solutions were widely adopted, it limited the depth of networks that could be trained successfully, as deeper networks were more susceptible to this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQkQTddgcVBX"
      },
      "source": [
        "#### 2. Why Vanishing Gradients Occur\n",
        "\n",
        "Several factors contribute to the vanishing gradient problem, especially in deep networks:\n",
        "\n",
        "##### 2.1. Activation Functions\n",
        "Certain activation functions, which were popular in earlier neural networks, have derivatives that saturate (become very small) for large input values (either positive or negative).\n",
        "* **Sigmoid Function ($\\sigma(x) = \\frac{1}{1+e^{-x}}$):** Its derivative is $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$. The maximum value of this derivative is 0.25 (at $x=0$). For inputs far from 0, the derivative approaches 0.\n",
        "* **Hyperbolic Tangent Function ($\\tanh(x)$):** Its derivative is $\\text{sech}^2(x) = 1 - \\tanh^2(x)$. The maximum value of this derivative is 1 (at $x=0$). For inputs far from 0 (e.g., $|x| > 3$), the derivative also approaches 0.\n",
        "\n",
        "##### 2.2. The Chain Rule in Backpropagation\n",
        "In a deep neural network, the gradient for a weight in an early layer is calculated by multiplying many terms together via the chain rule during backpropagation. These terms include the derivatives of the activation functions in subsequent layers and the weights of those layers.\n",
        "\n",
        "If a network has many layers, and many of these multiplicative terms are small (e.g., derivatives of sigmoid/tanh functions in their saturation regions, or weights with magnitudes less than 1), their product can decrease exponentially, leading to an extremely small gradient for the initial layers.\n",
        "\n",
        "Consider a simplified scenario: if you have $L$ layers and the average magnitude of the relevant derivative terms is $d_avg < 1$, the gradient signal for the first layer might be scaled by a factor roughly proportional to $(d_{avg})^L$. If $d_{avg}$ is significantly less than 1 and $L$ is large, this factor becomes vanishingly small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpeMVvJrcVBX"
      },
      "source": [
        "#### 3. Consequences of Vanishing Gradients\n",
        "\n",
        "* **Slow Training:** The most direct consequence is that the weights in the early layers of the network are updated very slowly, significantly prolonging the training time.\n",
        "* **Poor Performance / Ineffective Learning:** Because the early layers fail to learn meaningful feature representations from the data, the overall model performance suffers. The network essentially becomes \"stuck\" as the deeper layers cannot compensate for the lack of good initial features.\n",
        "* **Difficulty in Training Deep Networks:** It makes training very deep networks practically impossible with traditional activation functions and initialization schemes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKO3wWdIcVBX"
      },
      "source": [
        "#### 4. Illustrative Examples\n",
        "\n",
        "##### 4.1. Derivative of the `tanh` function\n",
        "The `tanh` function and its derivative illustrate how gradients can become small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewz3bDiwcVBX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch # Still used for a point-wise derivative example later if desired\n",
        "\n",
        "# --- Using NumPy for tanh and its derivative ---\n",
        "x_np = np.linspace(-7, 7, 400)\n",
        "tanh_x_np = np.tanh(x_np)\n",
        "dtanh_x_np = 1 - tanh_x_np**2 # Derivative of tanh(x) is 1 - tanh^2(x)\n",
        "\n",
        "plt.figure(figsize=(12, 5)) # Adjusted figsize as we only have one row of plots now\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x_np, tanh_x_np, label='tanh(x)')\n",
        "plt.title('tanh(x) Function (NumPy)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('tanh(x)')\n",
        "plt.grid(True)\n",
        "plt.axhline(0, color='black', lw=0.5)\n",
        "plt.axvline(0, color='black', lw=0.5)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x_np, dtanh_x_np, label=\"Derivative: 1 - tanh^2(x)\", color='orange')\n",
        "plt.title('Derivative of tanh(x) (NumPy)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel(\"f'(x)\")\n",
        "plt.grid(True)\n",
        "plt.axhline(0, color='black', lw=0.5)\n",
        "plt.axvline(0, color='black', lw=0.5)\n",
        "# Highlight regions where derivative is small\n",
        "plt.fill_between(x_np, 0, dtanh_x_np, where=(np.abs(x_np) > 2.5), color='orange', alpha=0.3, label='Small gradient region')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"--- NumPy Calculations ---\")\n",
        "print(f\"Value of tanh(4) = {np.tanh(4):.4f}\")\n",
        "print(f\"Derivative of tanh(x) at x=4: {1 - np.tanh(4)**2:.4f}\")\n",
        "print(f\"Value of tanh(-5) = {np.tanh(-5):.4f}\")\n",
        "print(f\"Derivative of tanh(x) at x=-5: {1 - np.tanh(-5)**2:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiTaPfx3cVBY"
      },
      "source": [
        "##### 4.2. Vanishing Gradient Problem with Sigmoid\n",
        "\n",
        "**How Sigmoid Contributes to Vanishing Gradients:**\n",
        "\n",
        "1.  **Derivative of the Sigmoid Function:**\n",
        "    As we've seen, the derivative of the Sigmoid function $S(x) = \\frac{1}{1 + e^{-x}}$ is:\n",
        "    $$S'(x) = S(x)(1 - S(x))$$\n",
        "\n",
        "2.  **Maximum Value of the Derivative:**\n",
        "    The maximum value of $S'(x)$ is $0.25$. This occurs at $x=0$, where $S(x)=0.5$, so $S'(0) = 0.5(1-0.5) = 0.25$.\n",
        "\n",
        "3.  **Derivative in Saturated Regions:**\n",
        "    When the input $x$ to the Sigmoid function is very large (positive or negative), the Sigmoid function \"saturates\" (its output $S(x)$ gets very close to 1 or 0):\n",
        "    * If $x$ is large and positive, $S(x) \\approx 1$. Then $S'(x) \\approx 1(1-1) = 0$.\n",
        "    * If $x$ is large and negative (e.g., $x < -5$), $S(x) \\approx 0$. Then $S'(x) \\approx 0(1-0) = 0$.\n",
        "    In these saturated regions, the local gradient $S'(x)$ is very close to zero.\n",
        "\n",
        "4.  **Gradient Propagation in Deep Networks:**\n",
        "    In a deep neural network, the gradient of the loss with respect to a weight in an early layer is a product of many terms, including the local gradients of the activation functions in all subsequent layers.\n",
        "    If we have $L$ layers using Sigmoid activations, the gradient signal for an early layer might be scaled by a factor roughly proportional to:\n",
        "    $$S'(z_L) \\times S'(z_{L-1}) \\times \\dots \\times S'(z_1)$$\n",
        "    where $S'(z_i)$ is the derivative of the Sigmoid function at the input $z_i$ of layer $i$.\n",
        "\n",
        "    Since each $S'(z_i) \\le 0.25$, multiplying many such terms together will cause the product to decrease exponentially:\n",
        "    * If all $S'(z_i)$ are at their maximum of $0.25$, then for $L$ layers, the product is $(0.25)^L$.\n",
        "    * For $L=5$, $(0.25)^5 \\approx 0.000976$.\n",
        "    * For $L=10$, $(0.25)^{10} \\approx 9.5 \\times 10^{-7}$.\n",
        "    * If any of the Sigmoid units are in their saturated regions, their local gradients $S'(z_i)$ will be much smaller than $0.25$, making the overall product even tinier and vanish even faster.\n",
        "\n",
        "**Consequences:**\n",
        "* **Slow Training:** Initial layers learn extremely slowly or get stuck.\n",
        "* **Poor Performance:** The network fails to learn complex patterns that require deep representations.\n",
        "\n",
        "**Reference:**\n",
        "https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf9GgypKcVBY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sigmoid function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    s_x = sigmoid(x)\n",
        "    return s_x * (1 - s_x)\n",
        "\n",
        "# 1. Plot the Sigmoid derivative to visualize its range\n",
        "x_vals_prime = np.linspace(-10, 10, 400)\n",
        "s_prime_vals = sigmoid_prime(x_vals_prime)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(x_vals_prime, s_prime_vals, label=\"$S'(x) = S(x)(1-S(x))$\")\n",
        "plt.title(\"Derivative of the Sigmoid Function $S'(x)$\")\n",
        "plt.xlabel(\"x (Input to Sigmoid)\")\n",
        "plt.ylabel(\"$S'(x)$ (Local Gradient)\")\n",
        "plt.axhline(0.25, color='red', linestyle='--', label='Max value = 0.25')\n",
        "plt.axhline(0, color='black', linestyle='-', lw=0.5)\n",
        "plt.axvline(0, color='gray', linestyle='--', lw=0.5)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(-0.05, 0.3) # Adjusted ylim to better see the curve\n",
        "plt.show()\n",
        "\n",
        "# 2. Demonstrate gradient values at different points\n",
        "print(\"--- Local Gradients (S'(x)) at different input values x ---\")\n",
        "x_points = np.array([-7, -5, -2, 0, 2, 5, 7]) # Inputs to Sigmoid units in a hypothetical network\n",
        "local_gradients_at_points = sigmoid_prime(x_points) # Renamed to avoid conflict\n",
        "\n",
        "for i, x_val in enumerate(x_points):\n",
        "    print(f\"For input x = {x_val:5.1f}, S(x) = {sigmoid(x_val):.4f}, S'(x) (local gradient) = {local_gradients_at_points[i]:.6f}\")\n",
        "\n",
        "# 3. Simulate and Plot Gradient Decay over Layers\n",
        "print(\"\\n--- Simulated Gradient Decay over Hypothetical Layers ---\")\n",
        "\n",
        "num_layers_max = 15 # Max number of layers to simulate for the plot\n",
        "layers = np.arange(1, num_layers_max + 1)\n",
        "\n",
        "# Scenarios for average local gradient values\n",
        "scenarios = {\n",
        "    \"Best Case (local grad=0.25)\": 0.25,\n",
        "    \"Moderate Case (local grad=0.15)\": 0.15,\n",
        "    \"Poor Case (local grad=0.05)\": 0.05,\n",
        "    \"Saturated (local grad=0.01)\": 0.01\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for scenario_name, avg_local_gradient in scenarios.items():\n",
        "    overall_gradient_factors = avg_local_gradient ** layers\n",
        "    plt.plot(layers, overall_gradient_factors, marker='o', linestyle='-', label=f'{scenario_name}')\n",
        "    # Print the last value for each scenario for comparison with previous text output\n",
        "    if layers.size > 0: # Ensure layers is not empty\n",
        "        print(f\"Scenario: {scenario_name}\")\n",
        "        for l_idx, l_val in enumerate(layers):\n",
        "            if l_val in [1, 3, 5, 10, 15]: # Print for specific layers\n",
        "                 print(f\"  After {l_val} layers, gradient factor ~ ({avg_local_gradient})^{l_val} = {overall_gradient_factors[l_idx]:.8e}\")\n",
        "\n",
        "\n",
        "plt.title(\"Simulated Gradient Factor Decay Across Layers\")\n",
        "plt.xlabel(\"Number of Layers\")\n",
        "plt.ylabel(\"Overall Gradient Factor (Magnitude)\")\n",
        "plt.xticks(layers) # Show all layer numbers if not too many, or adjust as needed\n",
        "plt.yscale('log') # Use log scale for y-axis to better visualize rapid decay\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\") # Grid for both major and minor ticks on log scale\n",
        "plt.ylim(bottom=1e-15) # Set a bottom limit for y-axis, especially for log scale\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nThe graph above visually demonstrates how quickly the gradient signal can diminish (vanish)\")\n",
        "print(\"as it propagates backward through increasing numbers of layers, especially when the\")\n",
        "print(\"local gradients (from Sigmoid derivatives) are small.\")\n",
        "print(\"Note the logarithmic scale on the y-axis to accommodate the rapid decrease.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-aksAdmcVBY"
      },
      "source": [
        "#### 5. Conclusion\n",
        "\n",
        "The vanishing gradient problem is a critical hurdle in training deep neural networks, stemming from the multiplicative effect of small derivatives during backpropagation, often exacerbated by certain activation functions. It leads to slow or stalled learning in the early layers of a network. Understanding this issue has spurred the development of various effective mitigation techniques, such as better activation functions (ReLU), improved weight initialization, normalization layers, and innovative network architectures (ResNets, LSTMs). These advancements have been pivotal in enabling the training of much deeper and more powerful neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hni7xPlKcVBY"
      },
      "source": [
        "## 12.1.3 Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6-jEn4kcVBb"
      },
      "source": [
        "### Exercise 1\n",
        " Consider a simple MLP with a single hidden layer of, say, $d$ dimensions in the hidden layer and a single output. Show that for any local minimum there are at least $d!$ equivalent solutions that behave identically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZozkHcPcVBb"
      },
      "source": [
        "Let:\n",
        "- $( \\mathbf{x} \\in \\mathbb{R}^n $): input vector\n",
        "- $( \\mathbf{W}_1 \\in \\mathbb{R}^{d \\times n} $): input-to-hidden weights\n",
        "- $( \\mathbf{b}_1 \\in \\mathbb{R}^d $): hidden layer bias\n",
        "- $( \\phi $): nonlinear activation function (applied element-wise)\n",
        "- $( \\mathbf{W}_2 \\in \\mathbb{R}^{1 \\times d} $): hidden-to-output weights\n",
        "- $( b_2 \\in \\mathbb{R} $): output bias\n",
        "\n",
        "The MLP function is:\n",
        "\n",
        "$[\n",
        "f(\\mathbf{x}) = \\mathbf{W}_2 \\cdot \\phi(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) + b_2\n",
        "$]\n",
        "\n",
        "---\n",
        "\n",
        "#### Permutation Symmetry\n",
        "\n",
        "Let $( \\pi $) be a permutation of the \\( d \\) hidden units.\n",
        "\n",
        "Define a **permutation matrix** $( P_\\pi \\in \\mathbb{R}^{d \\times d} $), such that:\n",
        "\n",
        "- $( \\mathbf{W}_1' = P_\\pi \\mathbf{W}_1 $)\n",
        "- $( \\mathbf{b}_1' = P_\\pi \\mathbf{b}_1 $)\n",
        "- $( \\mathbf{W}_2' = \\mathbf{W}_2 P_\\pi^\\top $)\n",
        "\n",
        "Then the permuted network computes:\n",
        "\n",
        "\\[\n",
        "\\begin{align*}\n",
        "f'(\\mathbf{x}) &= \\mathbf{W}_2' \\cdot \\phi(\\mathbf{W}_1' \\mathbf{x} + \\mathbf{b}_1') + b_2 \\\\\n",
        "&= \\mathbf{W}_2 P_\\pi^\\top \\cdot \\phi(P_\\pi \\mathbf{W}_1 \\mathbf{x} + P_\\pi \\mathbf{b}_1) + b_2 \\\\\n",
        "&= \\mathbf{W}_2 \\cdot \\phi(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) + b_2 = f(\\mathbf{x})\n",
        "\\end{align*}\n",
        "\\]\n",
        "\n",
        "So the function remains **unchanged** under such a permutation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz4gdydGcVBb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(0)  # For reproducibility\n",
        "\n",
        "# Define parameters\n",
        "d = 4            # Number of neurons in hidden layer\n",
        "n_input = 3      # Input feature dimension\n",
        "n_samples = 5    # Number of input samples\n",
        "\n",
        "# Generate random input data: shape (5 samples, 3 features)\n",
        "x = torch.randn(n_samples, n_input)\n",
        "\n",
        "# ---- Original MLP Weights and Biases ----\n",
        "# First layer: maps input (3D) to hidden layer (4D)\n",
        "W1 = torch.randn(d, n_input)   # Shape: (4, 3)\n",
        "b1 = torch.randn(d)            # Shape: (4,)\n",
        "\n",
        "# Second layer: maps hidden layer (4D) to output (1D)\n",
        "W2 = torch.randn(1, d)         # Shape: (1, 4)\n",
        "b2 = torch.randn(1)            # Shape: (1,)\n",
        "\n",
        "# ---- Original Network Output Function ----\n",
        "def original_net(x):\n",
        "    hidden = torch.relu(x @ W1.T + b1)  # Apply ReLU to hidden activations\n",
        "    output = hidden @ W2.T + b2         # Compute output\n",
        "    return output\n",
        "\n",
        "# Get output from original network\n",
        "y_orig = original_net(x)\n",
        "\n",
        "# ---- Create a Permutation of the Hidden Layer ----\n",
        "# This swaps hidden neurons arbitrarily\n",
        "perm = torch.randperm(d)           # A random permutation of [0, 1, 2, ..., d-1]\n",
        "\n",
        "# Apply the permutation:\n",
        "# - Reorder rows of W1 (each row is a hidden neuron’s weights)\n",
        "# - Reorder b1 entries (bias for each hidden neuron)\n",
        "# - Reorder columns of W2 (weights from hidden neurons to output)\n",
        "W1_perm = W1[perm]\n",
        "b1_perm = b1[perm]\n",
        "W2_perm = W2[:, perm]\n",
        "\n",
        "# ---- Permuted Network Output Function ----\n",
        "def permuted_net(x):\n",
        "    hidden = torch.relu(x @ W1_perm.T + b1_perm)\n",
        "    output = hidden @ W2_perm.T + b2  # Note: same output bias b2\n",
        "    return output\n",
        "\n",
        "# Get output from permuted network\n",
        "y_perm = permuted_net(x)\n",
        "\n",
        "# ---- Check if both outputs are identical ----\n",
        "# The outputs should be exactly the same because permutation doesn't affect result\n",
        "difference = torch.norm(y_orig - y_perm).item()\n",
        "print(\"Difference between original and permuted output:\", difference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWCVdqVdcVBb"
      },
      "source": [
        "### Exercise 2\n",
        "Assume that we have a symmetric random matrix $M$ where the entries $M_{ij} = M_{ji}$ are each drawn from some probability distribution $p_{ij}$. Furthermore, assume that $p_{ij}(x) = p_{ij}(-x)$, i.e., that the distribution is symmetric (see e.g., Wigner (1958) for details).\n",
        "\n",
        "1.  Prove that the distribution over eigenvalues is also symmetric. That is, for any eigenvector $v$, the probability that the associated eigenvalue $\\lambda$ satisfies $P(\\lambda > 0) = P(\\lambda < 0)$.\n",
        "2.  Why does the above *not* imply $P(\\lambda > 0) = 0.5$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKMkwWJcVBc"
      },
      "source": [
        "#### 1: Proving $P(\\lambda > 0) = P(\\lambda < 0)$\n",
        "\n",
        "##### Step 1: Consider the Matrix $-M$\n",
        "we take our original matrix $M$ and create a new matrix, called $-M$, by multiplying every entry in $M$ by -1.\n",
        "\n",
        "##### Step 2: Statistical Properties of $-M$\n",
        "Since the probability distribution for each entry $M_{ij}$ is symmetric (i.e., $p_{ij}(x) = p_{ij}(-x)$), the probability of $M_{ij}$ being some value $x$ is the same as it being $-x$.\n",
        "This means that an entry $-M_{ij}$ (from the matrix $-M$) will have the exact same probability distribution as an entry $M_{ij}$ (from the matrix $M$).\n",
        "Therefore, the matrix $-M$ is statistically indistinguishable from the matrix $M$. They both belong to the same \"family\" of random matrices, generated by the same underlying probability rules.\n",
        "\n",
        "##### Step 3: Eigenvalues of $M$ versus $-M$\n",
        "Let $\\lambda$ be an eigenvalue of $M$. This means there's a non-zero vector $v$ (an eigenvector) such that $Mv = \\lambda v$.\n",
        "Now, let's look at what happens with $-M$:\n",
        "$$ (-M)v = -(Mv) = -(\\lambda v) = (-\\lambda)v $$\n",
        "This equation shows that if $\\lambda$ is an eigenvalue of $M$, then $-\\lambda$ must be an eigenvalue of $-M$ (with the same eigenvector $v$).\n",
        "\n",
        "##### Step 4: Symmetry in the Eigenvalue Distribution\n",
        "We established two key points:\n",
        "1.  The matrices $M$ and $-M$ are statistically identical (they follow the same probability distribution for their creation).\n",
        "2.  If $\\lambda$ is an eigenvalue of $M$, then $-\\lambda$ is an eigenvalue of $-M$.\n",
        "\n",
        "Since $M$ and $-M$ are drawn from the same statistical process, their sets of eigenvalues must also follow the same overall probability distribution.\n",
        "This means that the distribution of eigenvalues for $M$ must be the same as the distribution of eigenvalues for $-M$.\n",
        "If the set of typical eigenvalues for $M$ is $\\{\\lambda_1, \\lambda_2, \\dots \\lambda_n\\}$, then the set of typical eigenvalues for $-M$ is $\\{-\\lambda_1, -\\lambda_2, \\dots -\\lambda_n\\}$.\n",
        "For these two sets to have the same distribution, the distribution of $\\lambda$ itself must be symmetric around 0. In simpler terms, the probability of observing an eigenvalue $\\lambda_{val}$ must be the same as observing an eigenvalue $-\\lambda_{val}$.\n",
        "\n",
        "##### Step 5: Conclusion for Part 1\n",
        "If the overall distribution of eigenvalues $\\lambda$ is symmetric around 0, it directly implies that the total probability of eigenvalues being positive must equal the total probability of eigenvalues being negative.\n",
        "\n",
        "Therefore, we can conclude that:\n",
        "$$ P(\\lambda > 0) = P(\\lambda < 0) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jNZqGOycVBc"
      },
      "source": [
        "#### 2: Why $P(\\lambda > 0) = P(\\lambda < 0)$ Does Not Imply $P(\\lambda > 0) = 0.5$\n",
        "\n",
        "##### Step 1: The Three Possibilities for an Eigenvalue\n",
        "Any eigenvalue $\\lambda$ can be classified in one of three ways:\n",
        "1.  **Positive**: $\\lambda > 0$\n",
        "2.  **Negative**: $\\lambda < 0$\n",
        "3.  **Zero**: $\\lambda = 0$\n",
        "\n",
        "These three possibilities are mutually exclusive (an eigenvalue can't be positive and negative at the same time) and exhaustive (there are no other options).\n",
        "\n",
        "##### Step 2: Probabilities Must Add Up to 1\n",
        "The sum of the probabilities of all possible outcomes must be 1 (or 100%). So:\n",
        "$$ P(\\lambda > 0) + P(\\lambda < 0) + P(\\lambda = 0) = 1 $$\n",
        "\n",
        "##### Step 3: Using the Result from Part 1\n",
        "From Part 1, we know that $P(\\lambda > 0) = P(\\lambda < 0)$. Let's call this common probability value '$X$'.\n",
        "So, $P(\\lambda > 0) = X$ and $P(\\lambda < 0) = X$.\n",
        "\n",
        "Substituting these into the equation from Step 2:\n",
        "$$ X + X + P(\\lambda = 0) = 1 $$\n",
        "This simplifies to:\n",
        "$$ 2X + P(\\lambda = 0) = 1 $$\n",
        "\n",
        "##### Step 4: Solving for $X$ (which is $P(\\lambda > 0)$)\n",
        "We want to find an expression for $X$. Rearranging the equation from Step 3:\n",
        "$$ 2X = 1 - P(\\lambda = 0) $$\n",
        "Dividing by 2:\n",
        "$$ X = \\frac{1 - P(\\lambda = 0)}{2} $$\n",
        "Since $X = P(\\lambda > 0)$, we have:\n",
        "$$ P(\\lambda > 0) = \\frac{1 - P(\\lambda = 0)}{2} $$\n",
        "\n",
        "##### Step 5: When Would $P(\\lambda > 0)$ Be Equal to 0.5?\n",
        "For $P(\\lambda > 0)$ to be equal to $0.5$, we would need:\n",
        "$$ 0.5 = \\frac{1 - P(\\lambda = 0)}{2} $$\n",
        "Multiply both sides by 2:\n",
        "$$ 1 = 1 - P(\\lambda = 0) $$\n",
        "For this equation to hold true, $P(\\lambda = 0)$ must be 0:\n",
        "$$ P(\\lambda = 0) = 0 $$\n",
        "\n",
        "##### Step 6: The Key Reason and Conclusion\n",
        "The equation $P(\\lambda > 0) = \\frac{1 - P(\\lambda = 0)}{2}$ shows that $P(\\lambda > 0)$ depends on $P(\\lambda = 0)$.\n",
        "\n",
        "$P(\\lambda > 0)$ is equal to $0.5$ **only if** the probability of an eigenvalue being exactly zero ($P(\\lambda = 0)$) is itself $0$.\n",
        "\n",
        "If there is any chance that an eigenvalue can be zero (i.e., if $P(\\lambda = 0) > 0$), then $1 - P(\\lambda = 0)$ will be less than 1. Consequently, $\\frac{1 - P(\\lambda = 0)}{2}$ will be less than $0.5$.\n",
        "\n",
        "Therefore, even though $P(\\lambda > 0) = P(\\lambda < 0)$, neither of these probabilities has to be $0.5$ if there's a non-zero chance that $\\lambda = 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csMXCe0UcVBc"
      },
      "source": [
        "#### Overall Conclusion\n",
        "\n",
        "1.  The symmetry of the probability distribution of the entries in a symmetric random matrix $M$ leads to a symmetric distribution for its eigenvalues, meaning $P(\\lambda > 0) = P(\\lambda < 0)$.\n",
        "2.  This equality does not force $P(\\lambda > 0)$ to be $0.5$ because the possibility of eigenvalues being exactly zero ($P(\\lambda = 0)$) must be accounted for. If $P(\\lambda = 0) > 0$, then $P(\\lambda > 0)$ will be less than $0.5$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar5Rx2T4cVBc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Parameters for the simulation ---\n",
        "matrix_dim = 2      # Dimension of the square symmetric matrix (e.g., 2x2)\n",
        "num_matrices = 20000 # Number of random matrices to generate for statistics\n",
        "\n",
        "entry_values = np.array([-1, 0, 1])\n",
        "entry_probabilities = np.array([0.3, 0.4, 0.3]) # Sum must be 1\n",
        "\n",
        "# --- Simulation ---\n",
        "all_eigenvalues = []\n",
        "\n",
        "print(f\"Generating {num_matrices} random {matrix_dim}x{matrix_dim} symmetric matrices...\")\n",
        "for _ in range(num_matrices):\n",
        "    matrix = np.zeros((matrix_dim, matrix_dim))\n",
        "    for i in range(matrix_dim):\n",
        "        for j in range(i, matrix_dim): # Fill upper triangle and diagonal\n",
        "            sample = np.random.choice(entry_values, p=entry_probabilities)\n",
        "            matrix[i, j] = sample\n",
        "            if i != j: # Ensure symmetry by copying to lower triangle\n",
        "                matrix[j, i] = sample\n",
        "\n",
        "    eigenvalues = np.linalg.eigvalsh(matrix) # For real symmetric matrices\n",
        "    all_eigenvalues.extend(eigenvalues)\n",
        "\n",
        "all_eigenvalues = np.array(all_eigenvalues)\n",
        "print(\"Simulation complete.\")\n",
        "\n",
        "# --- Analysis for Part 1: Symmetry of Eigenvalue Distribution ---\n",
        "# Define a small tolerance for comparing eigenvalues to zero numerically\n",
        "epsilon = 1e-9\n",
        "\n",
        "# Categorize non-zero eigenvalues\n",
        "positive_eigenvalues = all_eigenvalues[all_eigenvalues > epsilon]\n",
        "negative_eigenvalues = all_eigenvalues[all_eigenvalues < -epsilon]\n",
        "\n",
        "num_positive = len(positive_eigenvalues)\n",
        "num_negative = len(negative_eigenvalues)\n",
        "total_eigenvalues_count = len(all_eigenvalues)\n",
        "\n",
        "# Calculate empirical probabilities for non-zero parts\n",
        "if total_eigenvalues_count > 0:\n",
        "    prob_lambda_gt_0 = num_positive / total_eigenvalues_count\n",
        "    prob_lambda_lt_0 = num_negative / total_eigenvalues_count\n",
        "else:\n",
        "    prob_lambda_gt_0 = 0\n",
        "    prob_lambda_lt_0 = 0\n",
        "\n",
        "# --- Plotting for Part 1 (Histogram Only) ---\n",
        "plt.style.use('seaborn-v0_8-whitegrid') # You can change or remove this style if preferred\n",
        "\n",
        "plt.figure(figsize=(10, 6)) # Adjusted figsize for a single, clear plot\n",
        "plt.hist(all_eigenvalues, bins=100, density=True, color='deepskyblue', edgecolor='black', alpha=0.7)\n",
        "plt.title(f'Distribution of All Eigenvalues\\n(Illustrating Symmetry for Part 1)', fontsize=16)\n",
        "plt.xlabel('Eigenvalue (λ)', fontsize=14)\n",
        "plt.ylabel('Density', fontsize=14)\n",
        "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='λ = 0')\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y42aS0ticVBc"
      },
      "source": [
        "### Exercise 3\n",
        "What other challenges involved in deep learning optimization can you think of?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAVsuJz4cVBc"
      },
      "source": [
        "#### Exploding Gradients\n",
        "\n",
        "**What it is:** The opposite of vanishing gradients. Gradients become excessively large during backpropagation.\n",
        "\n",
        "**Why it's a problem:** Large gradients cause large weight updates, leading to unstable training. The optimizer might overshoot optimal points, the loss might oscillate wildly, or weights can grow so large they result in numerical overflow (`NaN` or `Inf` values), halting training.\n",
        "\n",
        "**Common approaches:** Gradient clipping (scaling down gradients if their norm exceeds a threshold), careful weight initialization, weight regularization, and using LSTM/GRU cells in RNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS1BjpxncVBc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simulate_gradient_propagation(num_steps, initial_gradient, weight_factor):\n",
        "    gradient_magnitudes = [initial_gradient]\n",
        "    current_gradient = initial_gradient\n",
        "    for _ in range(num_steps):\n",
        "        current_gradient *= weight_factor\n",
        "        # Cap/floor for practical plotting (real overflow/underflow would be NaN/Inf or precision loss)\n",
        "        if np.isinf(current_gradient) or np.abs(current_gradient) > 1e38: current_gradient = np.sign(current_gradient) * 1e38 if current_gradient != 0 else 1e38\n",
        "        elif np.abs(current_gradient) < 1e-38 and current_gradient != 0: current_gradient = np.sign(current_gradient) * 1e-38\n",
        "        gradient_magnitudes.append(current_gradient)\n",
        "    return gradient_magnitudes\n",
        "\n",
        "num_backward_steps = 25\n",
        "initial_gradient_magnitude = 1.0\n",
        "\n",
        "gradients_explode = simulate_gradient_propagation(num_backward_steps, initial_gradient_magnitude, 1.2) # Factor > 1\n",
        "gradients_stable = simulate_gradient_propagation(num_backward_steps, initial_gradient_magnitude, 1.0)  # Factor = 1\n",
        "gradients_vanish = simulate_gradient_propagation(num_backward_steps, initial_gradient_magnitude, 0.8) # Factor < 1\n",
        "steps = list(range(num_backward_steps + 1))\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(steps, gradients_explode, marker='o', linestyle='-', color='red', label='Exploding (Factor = 1.2)')\n",
        "plt.plot(steps, gradients_stable, marker='s', linestyle='--', color='green', label='Stable (Factor = 1.0)')\n",
        "plt.plot(steps, gradients_vanish, marker='x', linestyle=':', color='blue', label='Vanishing (Factor = 0.8)')\n",
        "plt.title('Conceptual Gradient Magnitude Propagation', fontsize=15)\n",
        "plt.xlabel('Number of Steps Backward (from output)', fontsize=12)\n",
        "plt.ylabel('Gradient Magnitude (Log Scale)', fontsize=12)\n",
        "plt.yscale('log')\n",
        "plt.xticks(np.arange(0, num_backward_steps + 1, 5))\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzjUdu3jcVBc"
      },
      "source": [
        "### Exercise 4\n",
        "Assume that you want to balance a (real) ball on a (real) saddle.\n",
        "\n",
        "Why is this hard?\n",
        "\n",
        "Can you exploit this effect also for optimization algorithms?\n",
        "\n",
        "#### Why is Balancing a Ball on a Saddle Hard?\n",
        "\n",
        "A **saddle surface** $( z = x^2 - y^2 $) curves:\n",
        "\n",
        "- **Upward in one direction** (e.g. along the x-axis),\n",
        "- **Downward in another direction** (e.g. along the y-axis).\n",
        "\n",
        "So, placing a ball at the center of the saddle:\n",
        "\n",
        "- It **rolls down** along the downward-curved direction,\n",
        "- It’s **trapped or balanced** in the upward-curved direction.\n",
        "\n",
        "This makes the saddle **unstable**:  \n",
        "> A tiny perturbation in the wrong direction (downward slope) causes the ball to accelerate away from the center.\n",
        "\n",
        "This is **why balancing a real ball on a saddle is difficult**—small deviations lead to large, uncontrollable motion.\n",
        "\n",
        "#### Can We Exploit Saddle Geometry in Optimization?\n",
        "\n",
        "Yes — understanding saddle geometry helps us design **better optimization algorithms**.\n",
        "\n",
        "- Since saddle points have **negative curvature** in some directions, we can:\n",
        "  - Use **second-order methods** (e.g. Newton’s method) that detect negative curvature.\n",
        "  - Add **noise** (as in SGD or Langevin dynamics) to escape flat saddle regions.\n",
        "  - Use algorithms like **Adam, RMSprop** that adapt learning rates directionally.\n",
        "\n",
        "##### Practical Strategies:\n",
        "- **Momentum** helps push through flat regions or escape slow convergence at saddles.\n",
        "- **Hessian-vector products** (used in trust-region or curvature-aware methods) can detect and escape saddles.\n",
        "\n",
        "So, just as a ball **naturally escapes** a saddle if nudged in the right direction,  \n",
        "**algorithms can exploit negative curvature** to avoid getting stuck during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SoXrp8scVBc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a saddle function: f(x, y) = x^2 - y^2\n",
        "def f(x, y):\n",
        "    return x**2 - y**2\n",
        "\n",
        "# Compute gradient\n",
        "def grad(x, y):\n",
        "    return np.array([2*x, -2*y])\n",
        "\n",
        "# Gradient descent on the saddle\n",
        "def gradient_descent(start, lr=0.1, steps=20):\n",
        "    path = [start]\n",
        "    x, y = start\n",
        "    for _ in range(steps):\n",
        "        g = grad(x, y)\n",
        "        x -= lr * g[0]\n",
        "        y -= lr * g[1]\n",
        "        path.append((x, y))\n",
        "    return np.array(path)\n",
        "\n",
        "# Start near the saddle point at (0, 0)\n",
        "path = gradient_descent(start=(1.0, 0.5), lr=0.1, steps=20)\n",
        "\n",
        "# Plot surface and path\n",
        "X, Y = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
        "Z = f(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, cmap='coolwarm', alpha=0.6)\n",
        "ax.plot(path[:, 0], path[:, 1], f(path[:, 0], path[:, 1]), color='black', marker='o', label=\"Gradient Descent Path\")\n",
        "ax.set_title('Gradient Descent on a Saddle Surface')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('f(x, y)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HZHVoEMcVBc"
      },
      "source": [
        "# 12.2 Convexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vzKSGcLcVBc"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "from mpl_toolkits import mplot3d\n",
        "from d2l import torch as d2l\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5i5xqpScVBc"
      },
      "source": [
        "## 12.2.1. Introduction to Convexity\n",
        "\n",
        "### 12.2.1.1 Problem Statement\n",
        "\n",
        "In optimization theory and computational algorithms, finding efficient and reliable methods for solving complex problems remains a significant challenge. A major issue is that many real-world optimization problems are inherently **nonconvex**, containing multiple local minima, making it hard to guarantee global optimality.\n",
        "\n",
        "Convexity simplifies this challenge considerably: a **convex optimization problem has no local minima except for the global one**. Hence, understanding and exploiting convexity becomes crucial.\n",
        "\n",
        "A set $( \\mathcal{X} $) is **convex** if for any two points $( a, b \\in \\mathcal{X} $), the line segment $( \\lambda a + (1 - \\lambda)b $) for $( \\lambda \\in [0,1] $) also belongs to $( \\mathcal{X} $).\n",
        "\n",
        "Convex functions \\( f: \\mathcal{X} \\to \\mathbb{R} \\) have a similar property:  \n",
        "$[\n",
        "f(\\lambda x + (1 - \\lambda)x') \\leq \\lambda f(x) + (1 - \\lambda)f(x')\n",
        "$]\n",
        "\n",
        "These properties make convex optimization problems easier to analyze, solve, and verify correctness.\n",
        "\n",
        "---\n",
        "\n",
        "### 12.2.1.2 Applications\n",
        "\n",
        "Convexity is widely used in many fields due to its desirable properties:\n",
        "\n",
        "- **Machine Learning & Deep Learning**:  \n",
        "  Optimization algorithms are validated on convex problems. Though neural networks are nonconvex, local convexity near minima supports effective convergence using SGD or Adam.\n",
        "\n",
        "- **Probabilistic Models**:  \n",
        "  **Jensen’s Inequality**, a key tool in convex analysis, is used in **variational inference**, enabling tractable approximations of complex likelihoods (e.g., in clustering or latent variable models).\n",
        "\n",
        "- **Signal & Image Processing**:  \n",
        "  Convex optimization is used in **compressed sensing** and **denoising** tasks by minimizing norms like $( \\ell_1 $) to recover sparse signals.\n",
        "\n",
        "- **Economics & Operations Research**:  \n",
        "  Resource allocation, logistics, and finance problems are efficiently solved using convex formulations (e.g., **portfolio optimization**).\n",
        "\n",
        "- **Control & Systems Theory**:  \n",
        "  Robust control and trajectory optimization benefit from convex guarantees on stability and performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 12.2.1.3 How Some CS Problems Have Been Solved Using Convexity Methods\n",
        "\n",
        "- **Support Vector Machines (SVMs)**:  \n",
        "  The optimization of SVMs is a **convex quadratic program**, leading to global optimal solutions for classification problems.\n",
        "\n",
        "- **Compressed Sensing**:  \n",
        "  Sparse signal recovery is posed as an **$( \\ell_1 $)-minimization problem**, a convex formulation that allows exact recovery from limited data.\n",
        "\n",
        "- **Variational Inference**:  \n",
        "  Using **Jensen’s Inequality**, one can bound complex expressions like the log-likelihood, facilitating tractable approximations in models like VAEs.\n",
        "\n",
        "- **Network Optimization**:  \n",
        "  Problems such as **routing, congestion control**, and **resource scheduling** are solved via convex formulations, ensuring efficiency and scalability.\n",
        "\n",
        "- **Optimization Algorithms**:  \n",
        "  Even when deep learning is nonconvex, convexity-based theory helps in designing algorithms (e.g., SGD, RMSProp) that converge well in practice by leveraging **local convexity**.\n",
        "---\n",
        "\n",
        "### 12.2.1.4 Definition of Convex Sets, Convex Functions, and Jensen's Inequality\n",
        "\n",
        "#### **Convex Sets**\n",
        "\n",
        "A **convex set** is a set $( S $) in a vector space that satisfies the following condition: for any two points $( x_1, x_2 \\in S $), the line segment connecting these two points lies entirely within the set $( S $). Mathematically:\n",
        "\n",
        "$[\n",
        "\\text{For all} \\, x_1, x_2 \\in S \\, \\text{and} \\, \\lambda \\in [0, 1], \\quad \\lambda x_1 + (1-\\lambda) x_2 \\in S\n",
        "$]\n",
        "\n",
        "This means that if you pick two points inside the set, every point on the straight line between them will also be inside the set.\n",
        "\n",
        "#### **Convex Functions**\n",
        "\n",
        "A function $( f $) is said to be **convex** on a domain $( D $) if, for any two points $( x_1, x_2 \\in D $) and any $( \\lambda \\in [0, 1] $), the following inequality holds:\n",
        "\n",
        "$[\n",
        "f(\\lambda x_1 + (1-\\lambda) x_2) \\leq \\lambda f(x_1) + (1-\\lambda) f(x_2)\n",
        "$]\n",
        "\n",
        "This property means that the **line segment** between any two points on the graph of the function lies **above** or **on** the graph itself, creating a \"bowl-shaped\" curve.\n",
        "\n",
        "- **Example**: The function $( f(x) = x^2 $) is convex because its graph is a parabola opening upwards.\n",
        "\n",
        "#### **Jensen's Inequality**\n",
        "\n",
        "**Jensen’s Inequality** is a fundamental inequality for **convex functions** that helps in bounding complex expressions. It states that for a convex function $( f $) and a random variable $( X $), the following inequality holds:\n",
        "\n",
        "$[\n",
        "f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)]\n",
        "$]\n",
        "\n",
        "This inequality has a very useful application in statistics, machine learning, and optimization:\n",
        "\n",
        "- **Interpretation**: The **expectation** of a convex function applied to a random variable is always **greater than or equal to** the convex function applied to the **expectation** of that random variable. This property is helpful when simplifying complex expressions, particularly in models like **Variational Autoencoders (VAEs)** where we work with log-likelihoods and in **variational inference**.\n",
        "\n",
        "#### **Why It Matters**\n",
        "- **Convex sets** and **convex functions** are crucial for ensuring optimization problems have **unique global minima**.\n",
        "- **Jensen’s Inequality** is a key tool in **variational inference**, helping approximate complex integrals and distributions in probabilistic models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2kETnA7cVBc"
      },
      "source": [
        "## 12.2.2. Properties\n",
        "\n",
        "Convex functions have many useful properties. We describe a few commonly-used ones below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOM-1OJ1cVBc"
      },
      "source": [
        "### 12.2.2.1. Local Minima Are Global Minima\n",
        "\n",
        "#### Problem Statement  \n",
        "If $f$ is a convex function, any local minimum is also a global minimum.\n",
        "\n",
        "#### Applications  \n",
        "- Guarantees global optimality in optimization.\n",
        "- Enables simple algorithms like gradient descent to find the best solution.\n",
        "\n",
        "#### CS Problems Solved  \n",
        "- SVM training: convex optimization → guarantees unique global solution.\n",
        "- Convex relaxations for NP-hard problems.\n",
        "- Convex regression and classification models (Logistic Regression, Lasso).\n",
        "\n",
        "#### Detail computations in step-by-step with examples\n",
        "\n",
        "**Problem:**  \n",
        "Prove that if $ f $ is convex on a domain $ D $, then any local minimum $ x^* \\in D $ is also a global minimum.\n",
        "\n",
        "\n",
        "**Step-by-step Explanation:**\n",
        "\n",
        "1. A function $ f $ is convex if for all $ x, y \\in D $ and $ \\theta \\in [0, 1] $:\n",
        "   $$\n",
        "   f(\\theta x + (1 - \\theta) y) \\leq \\theta f(x) + (1 - \\theta) f(y)\n",
        "   $$\n",
        "\n",
        "2. Suppose $ x^* $ is a local minimizer:\n",
        "   $$\n",
        "   f(x^*) \\leq f(x), \\quad \\text{for all } x \\text{ near } x^*\n",
        "   $$\n",
        "\n",
        "3. Since $ f $ is convex, we get the stronger condition:\n",
        "   $$\n",
        "   f(x^*) \\leq f(x), \\quad \\forall x \\in D \\implies x^* \\text{ is a global minimizer}\n",
        "   $$\n",
        "\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Minimize $ f(x) = (x - 1)^2 $\n",
        "\n",
        "- Take derivative:\n",
        "  $$\n",
        "  f'(x) = 2(x - 1) \\implies f'(x) = 0 \\text{ when } x = 1\n",
        "  $$\n",
        "\n",
        "- Second derivative:\n",
        "  $$\n",
        "  f''(x) = 2 > 0 \\implies f \\text{ is convex}\n",
        "  $$\n",
        "\n",
        "- So, $ x = 1 $ is a global minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9dvuv75cVBc"
      },
      "outputs": [],
      "source": [
        "f = lambda x: (x - 1) ** 2\n",
        "x = np.linspace(-2, 2, 100)\n",
        "segment = np.array([-1.5, 1.0])\n",
        "\n",
        "d2l.set_figsize()\n",
        "d2l.plot([x, segment], [f(x), f(segment)], 'x', 'f(x)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FN-sisYcVBd"
      },
      "source": [
        "### 12.2.2.2. Below Sets of Convex Functions Are Convex\n",
        "\n",
        "#### Problem Statement  \n",
        "Let $ f: \\mathbb{R}^n \\to \\mathbb{R} $ be a **convex function**.  \n",
        "Then for any $ \\alpha \\in \\mathbb{R} $, the **sublevel set** defined as:\n",
        "$$\n",
        "S_\\alpha = \\{ x \\in \\mathbb{R}^n \\mid f(x) \\leq \\alpha \\}\n",
        "$$\n",
        "is a **convex set**.\n",
        "\n",
        "That is, the set of all points where the function takes value at most $ \\alpha $ forms a convex region.\n",
        "\n",
        "#### Applications  \n",
        "- **Convex Feasible Regions**: In convex optimization problems, constraints often define sublevel sets — e.g., $ f(x) \\leq c $.\n",
        "- **Machine Learning**: Regularization penalties (like $ \\ell_1 $, $ \\ell_2 $) induce convex sublevel sets, leading to efficient solvers.\n",
        "- **Control Theory**: Safe operation zones are often defined via sublevel sets of Lyapunov or cost functions.\n",
        "- **Economics**: Utility or cost boundaries define feasible decision spaces that are convex via sublevel sets.\n",
        "\n",
        "\n",
        "#### Solved CS Problems  \n",
        "- Regularized Machine Learning Models\n",
        "- Robust Optimization\n",
        "- Multi-objective Optimization\n",
        "\n",
        "#### Detail computations in step-by-step with your example\n",
        "\n",
        "**Problem:**\n",
        "\n",
        "Show that the **sublevel set** of a convex function is convex.  \n",
        "That is, for a convex function $ f: \\mathbb{R}^n \\to \\mathbb{R} $,  \n",
        "the set:\n",
        "$$\n",
        "S_\\alpha = \\{ x \\in \\mathbb{R}^n \\mid f(x) \\leq \\alpha \\}\n",
        "$$\n",
        "is convex for any $ \\alpha \\in \\mathbb{R} $.\n",
        "\n",
        "**Step-by-step:**\n",
        "\n",
        "1. Let $ x_1, x_2 \\in S_\\alpha $, i.e.,  \n",
        "   $$\n",
        "   f(x_1) \\leq \\alpha \\quad \\text{and} \\quad f(x_2) \\leq \\alpha\n",
        "   $$\n",
        "\n",
        "2. For any $ \\theta \\in [0, 1] $, define $ x_\\theta = \\theta x_1 + (1 - \\theta)x_2 $\n",
        "\n",
        "3. Since $ f $ is convex:  \n",
        "   $$\n",
        "   f(x_\\theta) \\leq \\theta f(x_1) + (1 - \\theta) f(x_2) \\leq \\theta \\alpha + (1 - \\theta)\\alpha = \\alpha\n",
        "   $$\n",
        "\n",
        "4. Thus $ f(x_\\theta) \\leq \\alpha \\Rightarrow x_\\theta \\in S_\\alpha $\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "$$\n",
        "\\Rightarrow S_\\alpha \\text{ is convex}\n",
        "$$\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let $ f(x) = \\|x\\|^2 $, then the sublevel set  \n",
        "$$\n",
        "S_1 = \\{ x \\in \\mathbb{R}^2 \\mid \\|x\\|^2 \\leq 1 \\}\n",
        "$$\n",
        "is convex.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "213loWIgcVBd"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-3, 3, 400)\n",
        "f = x**2\n",
        "\n",
        "c = 1\n",
        "mask = f <= c\n",
        "\n",
        "plt.plot(x, f, label='f(x) = x²')\n",
        "plt.fill_between(x, f, c, where=mask, interpolate=True, color='skyblue', alpha=0.5, label='Sublevel set: f(x) ≤ 1')\n",
        "plt.axhline(c, color='gray', linestyle='--')\n",
        "plt.title('Sublevel Set of Convex Function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cbzw99hcVBd"
      },
      "source": [
        "### 12.2.2.3. Convexity and Second Derivatives\n",
        "\n",
        "#### Problem Statement  \n",
        "- If $ f''(x) \\geq 0 \\Rightarrow \\text{convex in 1D} $  \n",
        "- If $ \\nabla^2 f(x) \\succeq 0 \\Rightarrow \\text{convex in multi-D} $\n",
        "\n",
        "#### Applications  \n",
        "- Verify convexity via the Hessian.\n",
        "- Second-order optimization (Newton's method).\n",
        "- Smooth convex analysis.\n",
        "\n",
        "#### CS Problems Solved  \n",
        "- BFGS/L-BFGS for convex models.\n",
        "- Energy minimization in computer vision.\n",
        "- Smooth convex losses for better convergence.\n",
        "\n",
        "#### Detail computations in step-by-step with your example\n",
        "\n",
        "**Problem:**\n",
        "\n",
        "Show that a function $ f : \\mathbb{R} \\to \\mathbb{R} $ is convex iff $ f''(x) \\geq 0 $\n",
        "\n",
        "**Step-by-step:**\n",
        "\n",
        "1. Take the second derivative of the function  \n",
        "2. If $ f''(x) \\geq 0 $ for all $ x $, then the function is convex\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- $ f(x) = x^4 \\Rightarrow f''(x) = 12x^2 \\geq 0 \\Rightarrow $ convex  \n",
        "- $ f(x) = -x^2 \\Rightarrow f''(x) = -2 < 0 \\Rightarrow $ not convex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGWBHuJGcVBd"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-2, 2, 400)\n",
        "\n",
        "f1 = x**4\n",
        "f2 = -x**2\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x, f1, label=r'$f(x) = x^4$', color='blue')\n",
        "plt.title('Convex Function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x, f2, label=r'$f(x) = -x^2$', color='red')\n",
        "plt.title('Not Convex Function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2TTSvw9cVBd"
      },
      "source": [
        "## 12.2.3. Constraints\n",
        "A constraint $ c_i(\\mathbf{x}) \\leq 0 $ is convex if the feasible set $\n",
        "\\{ \\mathbf{x} \\mid c_i(\\mathbf{x}) \\leq 0 \\} $, is a convex set. \\\\\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQW591icVBd"
      },
      "source": [
        "### 12.2.3.1. Lagrangian and Duality\n",
        "\n",
        "#### Problem Statement  \n",
        "Construct the Lagrangian:  \n",
        "$$\n",
        "\\mathcal{L}(x, \\lambda, \\nu) = f(x) + \\sum_i \\lambda_i g_i(x) + \\sum_j \\nu_j h_j(x)\n",
        "$$\n",
        "and dual function:  \n",
        "$$\n",
        "g(\\lambda, \\nu) = \\inf_x \\mathcal{L}(x, \\lambda, \\nu)\n",
        "$$\n",
        "\n",
        "#### Applications  \n",
        "- Dual problem may be easier to solve.\n",
        "- Enables decomposition (e.g., ADMM).\n",
        "- Strong duality helps with tight optimality guarantees.\n",
        "\n",
        "#### CS Problems Solved  \n",
        "- SVM dual solves more efficiently with kernels.\n",
        "- Distributed optimization (federated learning).\n",
        "- Resource allocation in networks.\n",
        "\n",
        "#### Detail computations in step-by-step with your example\n",
        "\n",
        "**Problem:**\n",
        "\n",
        "Use the Lagrangian to solve: $$ \\min x^2 \\quad \\text{s.t. } x \\geq 1 $$ **Step-by-step:**\n",
        "1. Define constraint as $ g(x) = 1 - x \\leq 0 $\n",
        "2. Lagrangian: $$ \\mathcal{L}(x, \\lambda) = x^2 + \\lambda(1 - x) $$\n",
        "3. Minimize over $ x $: take derivative $$ \\frac{d\\mathcal{L}}{dx} = 2x - \\lambda = 0 \\Rightarrow x = \\frac{\\lambda}{2} $$\n",
        "4. Plug into constraint: $$ \\frac{\\lambda}{2} \\geq 1 \\Rightarrow \\lambda \\geq 2 $$\n",
        "5. Dual function: $ g(\\lambda) = \\mathcal{L}(\\lambda/2, \\lambda) $\n",
        "6. Maximize $ g(\\lambda) $ over $ \\lambda \\geq 0 $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkSA9lC-cVBd"
      },
      "outputs": [],
      "source": [
        "# Lagrangian function\n",
        "def lagrangian(x, lam):\n",
        "    return x**2 + lam * (1 - x)\n",
        "\n",
        "# Optimal x for given lambda (from derivative)\n",
        "def x_star(lam):\n",
        "    return lam / 2\n",
        "\n",
        "# Dual function: g(λ) = L(x*(λ), λ)\n",
        "def dual(lam):\n",
        "    x_opt = x_star(lam)\n",
        "    return lagrangian(x_opt, lam)\n",
        "\n",
        "# λ range\n",
        "lambdas = np.linspace(0, 5, 200)\n",
        "g_vals = [dual(l) for l in lambdas]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(lambdas, g_vals, label=\"Dual function $g(\\\\lambda)$\")\n",
        "plt.axvline(x=2, color='red', linestyle='--', label=\"$\\\\lambda=2$\")\n",
        "plt.xlabel(\"$\\\\lambda$\")\n",
        "plt.ylabel(\"$g(\\\\lambda)$\")\n",
        "plt.title(\"Dual Function for $\\\\min x^2$ s.t. $x \\\\geq 1$\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xedNoD-tcVBd"
      },
      "source": [
        "### 12.2.3.2. Penalty Methods and Regularization\n",
        "\n",
        "#### Problem Statement  \n",
        "Convert constraints into penalties:  \n",
        "$$\n",
        "\\min f(x) + \\lambda R(x)\n",
        "$$\n",
        "where $ R(x) $ is a convex regularizer.\n",
        "\n",
        "#### Applications  \n",
        "- Control overfitting (L1, L2 penalties).\n",
        "- Sparsity and structure enforcement.\n",
        "- Add soft constraints (e.g., fairness, size).\n",
        "\n",
        "#### CS Problems Solved  \n",
        "- Lasso: Enforces sparsity.\n",
        "- Elastic Net: Tradeoff between sparsity and smoothness.\n",
        "- Image denoising with Total Variation.\n",
        "\n",
        "#### Detail computations in step-by-step with your example\n",
        "\n",
        "**Problem:**\n",
        "\n",
        "Solve: $$ \\min x^2 \\quad \\text{s.t. } x \\geq 1 $$ using a penalty: $$ \\min x^2 + \\lambda \\cdot \\max(0, 1 - x)^2 $$\n",
        "**Step-by-step:**\n",
        "1. For $x \\geq 1$: penalty is 0 $\\Rightarrow$ original objective\n",
        "2. For $x < 1$: penalty increases rapidly\n",
        "3. Optimal occurs at $x = 1$ to avoid penalty and minimize $x^2$ </pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY61JbPmcVBd"
      },
      "outputs": [],
      "source": [
        "# Penalty parameter\n",
        "lam = 10\n",
        "\n",
        "# Objective function with penalty\n",
        "def f(x, lam):\n",
        "    return x**2 + lam * np.maximum(0, 1 - x)**2\n",
        "\n",
        "# Generate x values\n",
        "x_vals = np.linspace(-1, 3, 400)\n",
        "y_vals = f(x_vals, lam)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(x_vals, y_vals, label=f\"$x^2 + {lam} \\cdot \\max(0, 1 - x)^2$\", color='blue')\n",
        "plt.axvline(x=1, linestyle='--', color='gray', label=\"$x = 1$\")\n",
        "plt.title(\"Penalty Method Objective\")\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"Objective value\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxl-nuzPcVBd"
      },
      "source": [
        "### 12.3.3.3. Projection onto Convex Sets\n",
        "\n",
        "#### Problem Statement  \n",
        "Project $ x $ onto convex set $ C $:  \n",
        "$$\n",
        "\\text{Proj}_C(x) = \\arg\\min_{y \\in C} \\|x - y\\|^2\n",
        "$$\n",
        "\n",
        "#### Applications  \n",
        "- Projected Gradient Descent (PGD).\n",
        "- Feasibility search under hard constraints.\n",
        "- Proximal algorithms.\n",
        "\n",
        "#### CS Problems Solved  \n",
        "- Adversarial training with PGD.\n",
        "- Compressed sensing recovery.\n",
        "- Simplex projection for constrained ML models.\n",
        "\n",
        "#### Detail computations in step-by-step with your example\n",
        "\n",
        "**Problem:**\n",
        "\n",
        "Find: $$ \\text{Proj}_{[0,2]}(x) $$\n",
        "\n",
        "**Step-by-step:**\n",
        " 1. If $x \\in [0, 2] \\Rightarrow \\text{Proj}(x) = x$\n",
        " 2. If $x < 0 \\Rightarrow \\text{Proj}(x) = 0$\n",
        " 3. If $x > 2 \\Rightarrow \\text{Proj}(x) = 2$\n",
        "\n",
        "**Example:**\n",
        "- $x = 1 \\Rightarrow \\text{Proj}(1) = 1$\n",
        "- $x = 3 \\Rightarrow \\text{Proj}(3) = 2$\n",
        "- $x = -2 \\Rightarrow \\text{Proj}(-2) = 0$ </pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhLao24CcVBd"
      },
      "outputs": [],
      "source": [
        "def proj_onto_0_2(x):\n",
        "    if x < 0:\n",
        "        return 0\n",
        "    elif x > 2:\n",
        "        return 2\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "# Example cases\n",
        "examples = [1, 3, -2]\n",
        "for x in examples:\n",
        "    print(f\"Proj({x}) = {proj_onto_0_2(x)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJhsdyxBcVBd"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT_dcznwcVBd"
      },
      "source": [
        "####5. Prove that linear subspaces, i.e., $ X = \\{x∣Wx = b\\}$  , are convex sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuAHmcmPcVBd"
      },
      "source": [
        "$\n",
        "\\textbf{Solution:}\n",
        "$\n",
        "\n",
        "We want to show that for any two points $\\mathbf{x}_1, \\mathbf{x}_2 \\in \\mathcal{X}$, and any $\\lambda \\in [0,1]$\n",
        ", the convex combination\n",
        "$$\n",
        "\\mathbf{x}_\\lambda := \\lambda \\mathbf{x}_1 + (1 - \\lambda) \\mathbf{x}_2\n",
        "$$\n",
        "also belongs to \\( $\\mathcal{X}$ \\).\n",
        "\n",
        "Since $\\mathbf{x}_1, \\mathbf{x}_2 \\in \\mathcal{X}$, we have:\n",
        "$$\n",
        "\\mathbf{W} \\mathbf{x}_1 = \\mathbf{b}, \\quad \\mathbf{W} \\mathbf{x}_2 = \\mathbf{b}\n",
        "$$\n",
        "\n",
        "Then:\n",
        "$$\n",
        "\\mathbf{W} \\mathbf{x}_\\lambda = \\lambda \\mathbf{W} \\mathbf{x}_1 + (1 - \\lambda) \\mathbf{W} \\mathbf{x}_2 = \\lambda \\mathbf{b} + (1 - \\lambda) \\mathbf{b} = \\mathbf{b}\n",
        "$$\n",
        "\n",
        "Thus, \\( $\\mathbf{x}_\\lambda \\in \\mathcal{X}$ \\).\n",
        "\n",
        "$\n",
        "{\\textbf{ Conclusion: } \\text{The set } \\mathcal{X} \\text{ is convex.}}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xG_V40mcVBd"
      },
      "source": [
        "####6. Prove that in the case of linear subspaces with $\\mathbf{b} = \\mathbf{0}$, the projection $\\mathrm{Proj}_{\\mathcal{X}} \\mathbf{x}$ can be written as $\\mathbf{M} \\mathbf{x}$ for some matrix $\\mathbf{M}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R0SjRxycVBd"
      },
      "source": [
        "$\\textbf{Solution:}$\n",
        "\n",
        "Let $\\mathcal{X} \\subseteq \\mathbb{R}^n$ be a linear subspace. Then, there exists a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ whose columns form a basis for $\\mathcal{X}$, so we can write:\n",
        "$$\n",
        "\\mathcal{X} = \\{ \\mathbf{A} \\mathbf{z} \\mid \\mathbf{z} \\in \\mathbb{R}^k \\}\n",
        "$$\n",
        "\n",
        "We want to project a vector $\\mathbf{x} \\in \\mathbb{R}^n$ orthogonally onto $ \\mathcal{X} $. The projection $\\mathbf{p} = \\mathrm{Proj}_{\\mathcal{X}} \\mathbf{x} \\in \\mathcal{X}$ satisfies:\n",
        "$$\n",
        "\\mathbf{x} - \\mathbf{p} \\perp \\mathcal{X} \\quad \\Leftrightarrow \\quad \\mathbf{A}^\\top (\\mathbf{x} - \\mathbf{p}) = 0\n",
        "$$\n",
        "\n",
        "Since $\\mathbf{p} = \\mathbf{A} \\mathbf{z}$, we have:\n",
        "$$\n",
        "\\mathbf{A}^\\top (\\mathbf{x} - \\mathbf{A} \\mathbf{z}) = 0\n",
        "\\Rightarrow \\mathbf{A}^\\top \\mathbf{x} = \\mathbf{A}^\\top \\mathbf{A} \\mathbf{z}\n",
        "\\Rightarrow \\mathbf{z} = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{x}\n",
        "$$\n",
        "\n",
        "Substitute back:\n",
        "$$\n",
        "\\mathbf{p} = \\mathbf{A} \\mathbf{z}\n",
        "= \\mathbf{A} (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{x}\n",
        "$$\n",
        "\n",
        "Define:\n",
        "$$\n",
        "\\mathbf{M} := \\mathbf{A} (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top\n",
        "\\Rightarrow \\mathrm{Proj}_{\\mathcal{X}} \\mathbf{x} = \\mathbf{M} \\mathbf{x}\n",
        "$$\n",
        "\n",
        "$\\textbf{Conclusion:}$\n",
        "When $\\mathcal{X}$ is a linear subspace (i.e., when $\\mathbf{b} = \\mathbf{0} $), the projection $\\mathrm{Proj}_{\\mathcal{X}} \\mathbf{x}$ is a linear map and can be expressed as $\\mathbf{M} \\mathbf{x}$ for some matrix $\\mathbf{M} \\in \\mathbb{R}^{n \\times n} $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5WxuKQscVBe"
      },
      "source": [
        "####7. Show that for twice-differentiable convex functions $f$, we can write:\n",
        "$\n",
        "f(x + \\varepsilon) = f(x) + \\varepsilon f'(x) + \\frac{1}{2} \\varepsilon^2 f''(x + \\xi), \\quad \\text{for some } \\xi \\in [0, \\varepsilon]\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcZYpJ-McVBe"
      },
      "source": [
        "$\\textbf{Solution.}$\n",
        "\n",
        "Let us define a new function $g(\\varepsilon)$ as follows:\n",
        "$$\n",
        "g(\\varepsilon) := f(x + \\varepsilon)\n",
        "$$\n",
        "Then:\n",
        "$$\n",
        "g(0) = f(x), \\quad g'(\\varepsilon) = f'(x + \\varepsilon), \\quad g''(\\varepsilon) = f''(x + \\varepsilon)\n",
        "$$\n",
        "\n",
        "Now, we apply the $\\textbf{second-order Taylor expansion}$ of $g(\\varepsilon)$ at $\\varepsilon = 0$ with the remainder in Lagrange form:\n",
        "$$\n",
        "g(\\varepsilon) = g(0) + \\varepsilon g'(0) + \\frac{1}{2} \\varepsilon^2 g''(\\xi) \\quad \\text{for some } \\xi \\in (0, \\varepsilon)\n",
        "$$\n",
        "\n",
        "Substituting back $g(\\varepsilon) = f(x + \\varepsilon)$, we get:\n",
        "$$\n",
        "f(x + \\varepsilon) = f(x) + \\varepsilon f'(x) + \\frac{1}{2} \\varepsilon^2 f''(x + \\xi) \\quad \\text{for some } \\xi \\in (0, \\varepsilon)\n",
        "$$\n",
        "\n",
        "$\\textbf{Conclusion:} $\n",
        "For any twice-differentiable function $f$, including all $C^2$ convex functions, the following holds:\n",
        "$$\n",
        "f(x + \\varepsilon) = f(x) + \\varepsilon f'(x) + \\frac{1}{2} \\varepsilon^2 f''(x + \\xi), \\quad \\text{for some } \\xi \\in [0, \\varepsilon]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA90gzLucVBe"
      },
      "source": [
        "####8. Given a convex set $\\mathcal{X}$ and two vectors $\\mathbf{x}$ and $\\mathbf{y}$, prove that projections never increase distances, i.e.,\n",
        "$$\n",
        "\\|\\mathbf{x} - \\mathbf{y}\\| \\geq \\|\\text{Proj}_{\\mathcal{X}}(\\mathbf{x}) - \\text{Proj}_{\\mathcal{X}}(\\mathbf{y})\\|.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHrC5PR4cVBe"
      },
      "source": [
        "$\\textbf{Solution.}$\n",
        "\n",
        "Let $\\mathbf{p} := \\text{Proj}_{\\mathcal{X}}(\\mathbf{x})$ and $\\mathbf{q} := \\text{Proj}_{\\mathcal{X}}(\\mathbf{y})$ be the orthogonal projections of $\\mathbf{x}$ and $\\mathbf{y}$ onto the convex set $\\mathcal{X}$. A fundamental property of projections onto convex sets is the **firm non-expansiveness** property, which implies:\n",
        "\n",
        "$$\n",
        "\\langle \\mathbf{x} - \\mathbf{y}, \\mathbf{p} - \\mathbf{q} \\rangle \\geq \\|\\mathbf{p} - \\mathbf{q}\\|^2.\n",
        "$$\n",
        "\n",
        "Now, apply the Cauchy-Schwarz inequality to the left-hand side:\n",
        "\n",
        "$$\n",
        "\\langle \\mathbf{x} - \\mathbf{y}, \\mathbf{p} - \\mathbf{q} \\rangle \\leq \\|\\mathbf{x} - \\mathbf{y}\\| \\cdot \\|\\mathbf{p} - \\mathbf{q}\\|.\n",
        "$$\n",
        "\n",
        "Combining both inequalities:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{p} - \\mathbf{q}\\|^2 \\leq \\|\\mathbf{x} - \\mathbf{y}\\| \\cdot \\|\\mathbf{p} - \\mathbf{q}\\|.\n",
        "$$\n",
        "\n",
        "Divide both sides by $\\|\\mathbf{p} - \\mathbf{q}\\|$ (if non-zero), we obtain:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{p} - \\mathbf{q}\\| \\leq \\|\\mathbf{x} - \\mathbf{y}\\|.\n",
        "$$\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$$\n",
        "\\|\\text{Proj}_{\\mathcal{X}}(\\mathbf{x}) - \\text{Proj}_{\\mathcal{X}}(\\mathbf{y})\\| \\leq \\|\\mathbf{x} - \\mathbf{y}\\|.\n",
        "$$\n",
        "\n",
        "$\\textbf{Conclusion:}$ Projection onto a convex set is a non-expansive operation in Euclidean space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wMCU8aUcVBe"
      },
      "source": [
        "## 12.2.4. Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22v6WdcRcVBe"
      },
      "source": [
        "### Exercise 1\n",
        "1. Assume that we want to verify convexity of a set by drawing all lines between points within the set and checking whether the lines are contained.\n",
        "    1. Prove that it is sufficient to check only the points on the boundary.\n",
        "    2. Prove that it is sufficient to check only the vertices of the set.\n",
        "#### Exercise 1.1\n",
        "\n",
        "Step 1: Suppose we have a closed set in the plane, like a polygon.\n",
        "\n",
        "Step 2: By definition, a set is convex if the line between any two points in the set stays entirely within the set.\n",
        "\n",
        "Step 3: Now, we focus only on the points located on the boundary of the set.\n",
        "\n",
        "Step 4: Assume that every line segment connecting any two boundary points lies completely inside the set.\n",
        "\n",
        "Step 5: Take any two interior points in the set—we want to prove the line between them also stays inside.\n",
        "\n",
        "Step 6: Since interior points can be represented as combinations of boundary points (e.g., via triangulation), the line between them is composed of segments between boundary points.\n",
        "\n",
        "Step 7: Therefore, the line between any two interior points also lies inside the set if all boundary-to-boundary segments do.\n",
        "\n",
        "=> Thus, if we verify that all lines between boundary points stay inside the set, then the set is convex.\n",
        "\n",
        "\n",
        "\n",
        "#### Exercise 1.2\n",
        "\n",
        "Step 1: Suppose we are working with a closed 2D shape represented as a polygon.\n",
        "\n",
        "Step 2: The polygon is defined by a finite set of ordered points called vertices.\n",
        "\n",
        "Step 3: By definition, a polygon is convex if the line between any two points inside the polygon lies entirely within it.\n",
        "\n",
        "Step 4: We now focus only on the vertices of the polygon, ignoring interior and edge points.\n",
        "\n",
        "Step 5: Generate all possible line segments between pairs of vertices.\n",
        "\n",
        "Step 6: For each segment, check whether it lies entirely inside or on the boundary of the polygon.\n",
        "\n",
        "Step 7: If any segment between two vertices goes outside the polygon, then the polygon is not convex.\n",
        "\n",
        "Step 8: If all vertex-to-vertex segments lie within or on the polygon, then no \"inward dents\" exist.\n",
        "\n",
        "=> Therefore, the polygon is convex if and only if all segments between its vertices lie inside it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLCY-2idcVBe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from shapely.geometry import Polygon, LineString\n",
        "import itertools\n",
        "\n",
        "def is_convex_by_vertices(points):\n",
        "    poly = Polygon(points)\n",
        "    if not poly.is_valid:\n",
        "        raise ValueError(\"Invalid polygon\")\n",
        "\n",
        "    for p1, p2 in itertools.combinations(points, 2):\n",
        "        line = LineString([p1, p2])\n",
        "        if not poly.contains(line) and not poly.boundary.contains(line):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Example 1: Convex polygon\n",
        "convex_pts = [(0, 0), (2, 0), (2, 2), (0, 2)]\n",
        "\n",
        "# Example 2: Non-convex polygon (concave)\n",
        "non_convex_pts = [(0, 0), (2, 0), (1, 1), (2, 2), (0, 2)]\n",
        "\n",
        "print(\"Convex polygon:\", is_convex_by_vertices(convex_pts))        # True\n",
        "print(\"Non-convex polygon:\", is_convex_by_vertices(non_convex_pts))  # False\n",
        "\n",
        "# Plotting function\n",
        "def plot_polygon(points, title):\n",
        "    poly = Polygon(points)\n",
        "    x, y = poly.exterior.xy\n",
        "    plt.plot(x, y, marker='o')\n",
        "    plt.fill(x, y, alpha=0.3)\n",
        "    plt.title(title)\n",
        "    plt.axis('equal')\n",
        "    plt.show()\n",
        "\n",
        "plot_polygon(convex_pts, \"Convex Polygon\")\n",
        "plot_polygon(non_convex_pts, \"Non-Convex Polygon\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz93Hqr_cVBe"
      },
      "source": [
        "### Exercise 2\n",
        "Denote by $\\mathcal{B}_p[r] \\stackrel{\\textrm{def}}{=} \\{\\mathbf{x} | \\mathbf{x} \\in \\mathbb{R}^d \\textrm{ and } \\|\\mathbf{x}\\|_p \\leq r\\}$ the ball of radius $r$ using the $p$-norm. Prove that $\\mathcal{B}_p[r]$ is convex for all $p \\geq 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR1QL0lqcVBe"
      },
      "source": [
        "#### Proof\n",
        "\n",
        "Let $( \\mathbf{x}, \\mathbf{y} \\in \\mathcal{B}_p[r]$), which means:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{x}\\|_p \\le r \\quad \\text{and} \\quad \\|\\mathbf{y}\\|_p \\le r\n",
        "$$\n",
        "\n",
        "Take any $( \\lambda \\in [0, 1] $), and define:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = \\lambda \\mathbf{x} + (1 - \\lambda)\\mathbf{y}\n",
        "$$\n",
        "\n",
        "We want to prove:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{z}\\|_p \\le r \\quad \\Rightarrow \\quad \\mathbf{z} \\in \\mathcal{B}_p[r]\n",
        "$$\n",
        "\n",
        "\n",
        "###Apply Minkowski's Inequality\n",
        "\n",
        "Minkowski’s inequality (valid for all $( p \\ge 1 $)) states that:\n",
        "\n",
        "$$\n",
        "\\|\\lambda \\mathbf{x} + (1 - \\lambda)\\mathbf{y}\\|_p \\le \\lambda \\|\\mathbf{x}\\|_p + (1 - \\lambda)\\|\\mathbf{y}\\|_p\n",
        "$$\n",
        "\n",
        "Using the assumption $( \\|\\mathbf{x}\\|_p \\le r $), $( \\|\\mathbf{y}\\|_p \\le r $), we have:\n",
        "\n",
        "$$\n",
        "\\lambda \\|\\mathbf{x}\\|_p + (1 - \\lambda)\\|\\mathbf{y}\\|_p \\le \\lambda r + (1 - \\lambda) r = r\n",
        "$$\n",
        "\n",
        "Thus:\n",
        "\n",
        "$$\n",
        "\\|\\lambda \\mathbf{x} + (1 - \\lambda)\\mathbf{y}\\|_p \\le r\n",
        "$$\n",
        "\n",
        "So $( \\mathbf{z} \\in \\mathcal{B}_p[r] $), proving the convexity.\n",
        "\n",
        "\n",
        "###Conclusion\n",
        "\n",
        "Hence, $( \\mathcal{B}_p[r] $) is a convex set for all $( p \\ge 1 $)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytn1uZy4cVBe"
      },
      "outputs": [],
      "source": [
        "# Visualize p-norm balls for various\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_lp_ball(p, r=1, num_points=100):\n",
        "    \"\"\"\n",
        "    Plot the boundary of the Lp-norm ball in 2D.\n",
        "\n",
        "    Parameters:\n",
        "    - p: The order of the norm (p >= 1 for convexity).\n",
        "    - r: Radius of the norm ball (default = 1).\n",
        "    - num_points: Number of angles to sample for smooth curve.\n",
        "    \"\"\"\n",
        "    # Generate angles from 0 to 2*pi (full circle)\n",
        "    theta = np.linspace(0, 2 * np.pi, num_points)\n",
        "\n",
        "    # Corresponding (x, y) points on the unit circle\n",
        "    x = np.cos(theta)\n",
        "    y = np.sin(theta)\n",
        "\n",
        "    # Compute the p-norm of each point on the unit circle\n",
        "    # This gives how far the point is from the origin in Lp norm\n",
        "    norm = (np.abs(x)**p + np.abs(y)**p)**(1/p)\n",
        "\n",
        "    # Scale each point so that it lies on the boundary of the Lp-ball of radius r\n",
        "    x_scaled = r * x / norm\n",
        "    y_scaled = r * y / norm\n",
        "\n",
        "    # Plot the resulting boundary\n",
        "    plt.plot(x_scaled, y_scaled, label=f\"p = {p}\")\n",
        "\n",
        "# Set up the plot\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Plot unit Lp balls for different values of p\n",
        "for p in [1, 1.5, 2, 5, 10]:\n",
        "    plot_lp_ball(p)\n",
        "\n",
        "# Formatting the plot\n",
        "plt.title(\"Unit Balls for Different $\\\\ell_p$ Norms (in 2D)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.axis('equal')      # Ensure x and y axes are scaled equally\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3pcvRSscVBe"
      },
      "source": [
        "### Exercise 3\n",
        "Given convex functions $f$ and $g$, show that $\\mathrm{max}(f, g)$ is convex, too. Prove that $\\mathrm{min}(f, g)$ is not convex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVAPFNLtcVBe"
      },
      "source": [
        "#### Part 1: Prove $\\max(f, g)$ is convex\n",
        "\n",
        "Let $h(x) = \\max\\bigl(f(x), g(x)\\bigr)$.  \n",
        "By the definition of convexity, we need to show that for all $x, x'$ and all $\\lambda \\in [0, 1]$,\n",
        "\n",
        "$$\n",
        "h\\bigl(\\lambda x + (1 - \\lambda)x'\\bigr)\n",
        "\\;\\le\\;\n",
        "\\lambda\\,h(x)\\;+\\;(1 - \\lambda)\\,h(x')\\,.\n",
        "$$\n",
        "\n",
        "Since $f$ and $g$ are convex, we have\n",
        "\n",
        "$$\n",
        "f\\bigl(\\lambda x + (1 - \\lambda)x'\\bigr)\n",
        "\\;\\le\\;\n",
        "\\lambda f(x) + (1 - \\lambda)f(x'),\n",
        "$$\n",
        "\n",
        "$$\n",
        "g\\bigl(\\lambda x + (1 - \\lambda)x'\\bigr)\n",
        "\\;\\le\\;\n",
        "\\lambda g(x) + (1 - \\lambda)g(x').\n",
        "$$\n",
        "\n",
        "Hence\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h\\bigl(\\lambda x + (1 - \\lambda)x'\\bigr)\n",
        "&= \\max\\bigl(f(\\lambda x + (1 - \\lambda)x'),\\,g(\\lambda x + (1 - \\lambda)x')\\bigr)\\\\\n",
        "&\\le \\max\\bigl(\\lambda f(x) + (1 - \\lambda)f(x'),\\;\\lambda g(x) + (1 - \\lambda)g(x')\\bigr)\\\\\n",
        "&\\le \\lambda\\,\\max\\bigl(f(x), g(x)\\bigr)\\;+\\;(1 - \\lambda)\\,\\max\\bigl(f(x'), g(x')\\bigr)\\\\\n",
        "&= \\lambda\\,h(x)\\;+\\;(1 - \\lambda)\\,h(x').\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Thus $h(x)=\\max(f(x),g(x))$ is convex.\n",
        "\n",
        "#### Part 2: Show $\\min(f, g)$ is not necessarily convex\n",
        "\n",
        "Consider the convex functions\n",
        "$$\n",
        "f(x)=x,\\quad g(x)=-x.\n",
        "$$\n",
        "\n",
        "Then\n",
        "\n",
        "$$\n",
        "\\min\\bigl(f(x),g(x)\\bigr)\n",
        "=\\min\\bigl(x,-x\\bigr)\n",
        "=-|x|,\n",
        "$$\n",
        "\n",
        "and $-\\,|x|$ is **concave**, not convex.  \n",
        "Therefore $\\min(f,g)$ need not be convex.\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "- The pointwise maximum of two convex functions is always convex.  \n",
        "- The pointwise minimum of two convex functions is not necessarily convex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ghTgyAYcVBe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(-2, 2, 400)\n",
        "\n",
        "# Convex functions\n",
        "f = x\n",
        "g = -x\n",
        "\n",
        "# Max and min of convex functions\n",
        "h_max = np.maximum(f, g)      # convex\n",
        "h_min = np.minimum(f, g)      # non-convex\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Plot max(f, g)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x, f, '--', label='f(x) = x')\n",
        "plt.plot(x, g, '--', label='g(x) = -x')\n",
        "plt.plot(x, h_max, 'r', label='max(f, g)')\n",
        "plt.title(\"max(f, g) is convex\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot min(f, g)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x, f, '--', label='f(x) = x')\n",
        "plt.plot(x, g, '--', label='g(x) = -x')\n",
        "plt.plot(x, h_min, 'r', label='min(f, g)')\n",
        "plt.title(\"min(f, g) is NOT convex\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8EbYRn9cVBf"
      },
      "source": [
        "### Exercise 4\n",
        "Prove that the normalization of the softmax function is convex. More specifically prove the convexity of $f(x) = \\log \\sum_i \\exp(x_i)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KBrOeqBcVBf"
      },
      "source": [
        "#### Proof via Hessian\n",
        "\n",
        "We will show that the Hessian of $f$ is positive semidefinite.\n",
        "\n",
        "**Step 1: Compute the gradient.**  \n",
        "Let\n",
        "$$\n",
        "S(\\mathbf{x}) = \\sum_{i=1}^n e^{x_i},\n",
        "\\qquad\n",
        "p_i(\\mathbf{x}) = \\frac{e^{x_i}}{S(\\mathbf{x})}\\,.\n",
        "$$\n",
        "Then\n",
        "$$\n",
        "\\nabla f(\\mathbf{x})\n",
        "= \\frac{1}{S(\\mathbf{x})}\\,\\bigl(e^{x_1},\\dots,e^{x_n}\\bigr)^\\top\n",
        "= \\bigl(p_1(\\mathbf{x}),\\dots,p_n(\\mathbf{x})\\bigr)^\\top.\n",
        "$$\n",
        "\n",
        "**Step 2: Compute the Hessian.**  \n",
        "Differentiate again:\n",
        "$$\n",
        "\\frac{\\partial^2 f}{\\partial x_i\\,\\partial x_j}\n",
        "= \\frac{\\partial p_i}{\\partial x_j}\n",
        "= \\begin{cases}\n",
        "p_i\\,(1 - p_i), & i = j,\\\\\n",
        "-\\,p_i\\,p_j,    & i \\neq j.\n",
        "\\end{cases}\n",
        "$$\n",
        "Hence in matrix form,\n",
        "$$\n",
        "\\nabla^2 f(\\mathbf{x})\n",
        "= \\operatorname{diag}\\bigl(p(\\mathbf{x})\\bigr)\\;\n",
        "-\\;p(\\mathbf{x})\\,p(\\mathbf{x})^\\top.\n",
        "$$\n",
        "\n",
        "**Step 3: Show positive semidefiniteness.**  \n",
        "For any $z\\in\\mathbb{R}^n$,\n",
        "$$\n",
        "z^\\top \\nabla^2 f(\\mathbf{x})\\,z\n",
        "= \\sum_{i=1}^n p_i\\,z_i^2 \\;-\\;\\Bigl(\\sum_{i=1}^n p_i\\,z_i\\Bigr)^{\\!2}\n",
        "= \\mathbb{E}[Z^2] - \\bigl(\\mathbb{E}[Z]\\bigr)^2\n",
        "\\;\\ge\\; 0,\n",
        "$$\n",
        "where $Z$ is a random variable taking value $z_i$ with probability $p_i$.  \n",
        "Variance is nonnegative, so $z^\\top \\nabla^2 f(\\mathbf{x})z\\ge0$.\n",
        "\n",
        "---\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "$\\nabla^2 f(\\mathbf{x})$ is positive semidefinite for all $\\mathbf{x}$,  \n",
        "therefore $f(\\mathbf{x}) = \\log\\sum_i e^{x_i}$ is convex. \\(\\quad\\blacksquare\\)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psCjYOwBcVBf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define log-sum-exp function in 2D\n",
        "def logsumexp(x1, x2):\n",
        "    return np.log(np.exp(x1) + np.exp(x2))\n",
        "\n",
        "# Grid over 2D space\n",
        "x = np.linspace(-5, 5, 200)\n",
        "y = np.linspace(-5, 5, 200)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = logsumexp(X, Y)\n",
        "\n",
        "# 3D surface plot\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, cmap='viridis')\n",
        "ax.set_title('$f(x_1, x_2) = \\log(\\exp(x_1) + \\exp(x_2))$ is convex')\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_zlabel('$f(x)$')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqjCsspqcVBf"
      },
      "source": [
        "### Exercise 5\n",
        "Prove that linear subspaces, i.e., $ X = \\{x∣Wx = b\\}$  , are convex sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fO-cpRQcVBf"
      },
      "source": [
        "$\n",
        "\\textbf{Solution:}\n",
        "$\n",
        "\n",
        "We want to show that for any two points $\\mathbf{x}_1, \\mathbf{x}_2 \\in \\mathcal{X}$, and any $\\lambda \\in [0,1]$\n",
        ", the convex combination\n",
        "$$\n",
        "\\mathbf{x}_\\lambda := \\lambda \\mathbf{x}_1 + (1 - \\lambda) \\mathbf{x}_2\n",
        "$$\n",
        "also belongs to \\( $\\mathcal{X}$ \\).\n",
        "\n",
        "Since $\\mathbf{x}_1, \\mathbf{x}_2 \\in \\mathcal{X}$, we have:\n",
        "$$\n",
        "\\mathbf{W} \\mathbf{x}_1 = \\mathbf{b}, \\quad \\mathbf{W} \\mathbf{x}_2 = \\mathbf{b}\n",
        "$$\n",
        "\n",
        "Then:\n",
        "$$\n",
        "\\mathbf{W} \\mathbf{x}_\\lambda = \\lambda \\mathbf{W} \\mathbf{x}_1 + (1 - \\lambda) \\mathbf{W} \\mathbf{x}_2 = \\lambda \\mathbf{b} + (1 - \\lambda) \\mathbf{b} = \\mathbf{b}\n",
        "$$\n",
        "\n",
        "Thus, \\( $\\mathbf{x}_\\lambda \\in \\mathcal{X}$ \\).\n",
        "\n",
        "$\n",
        "{\\textbf{ Conclusion: } \\text{The set } \\mathcal{X} \\text{ is convex.}}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZDOCaRucVBf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define W and b\n",
        "W = np.array([1, 2])\n",
        "b = 3\n",
        "\n",
        "# Create x values\n",
        "x_vals = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Solve for y in the constraint: x + 2y = 3 --> y = (3 - x)/2\n",
        "y_vals = (b - W[0] * x_vals) / W[1]\n",
        "\n",
        "# Plot the line representing the affine set\n",
        "plt.plot(x_vals, y_vals, label=r'$x + 2y = 3$', color='blue')\n",
        "\n",
        "# Plot some convex combinations\n",
        "x1 = np.array([0, 1.5])\n",
        "x2 = np.array([4, -0.5])\n",
        "lambdas = np.linspace(0, 1, 100)\n",
        "combos = np.array([l * x1 + (1 - l) * x2 for l in lambdas])\n",
        "plt.plot([x1[0], x2[0]], [x1[1], x2[1]], 'ro', label='Endpoints')\n",
        "plt.plot(combos[:,0], combos[:,1], 'r--', label='Convex combinations')\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.title('Affine set $x + 2y = 3$ is convex')\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y$')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxzqlXnYcVBf"
      },
      "source": [
        "### Exercise 6\n",
        "Prove that in the case of linear subspaces with $\\mathbf{b} = \\mathbf{0}$, the projection $\\mathrm{Proj}_{\\mathcal{X}} \\mathbf{x}$ can be written as $\\mathbf{M} \\mathbf{x}$ for some matrix $\\mathbf{M}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCyCY4w9cVBf"
      },
      "source": [
        "$\\textbf{Solution:}$\n",
        "\n",
        "Let $\\mathcal{X} \\subseteq \\mathbb{R}^n$ be a linear subspace. Then, there exists a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ whose columns form a basis for $\\mathcal{X}$, so we can write:\n",
        "$$\n",
        "\\mathcal{X} = \\{ \\mathbf{A} \\mathbf{z} \\mid \\mathbf{z} \\in \\mathbb{R}^k \\}\n",
        "$$\n",
        "\n",
        "We want to project a vector $\\mathbf{x} \\in \\mathbb{R}^n$ orthogonally onto $ \\mathcal{X} $. The projection $\\mathbf{p} = \\mathrm{Proj}_{\\mathcal{X}} \\mathbf{x} \\in \\mathcal{X}$ satisfies:\n",
        "$$\n",
        "\\mathbf{x} - \\mathbf{p} \\perp \\mathcal{X} \\quad \\Leftrightarrow \\quad \\mathbf{A}^\\top (\\mathbf{x} - \\mathbf{p}) = 0\n",
        "$$\n",
        "\n",
        "Since $\\mathbf{p} = \\mathbf{A} \\mathbf{z}$, we have:\n",
        "$$\n",
        "\\mathbf{A}^\\top (\\mathbf{x} - \\mathbf{A} \\mathbf{z}) = 0\n",
        "\\Rightarrow \\mathbf{A}^\\top \\mathbf{x} = \\mathbf{A}^\\top \\mathbf{A} \\mathbf{z}\n",
        "\\Rightarrow \\mathbf{z} = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{x}\n",
        "$$\n",
        "\n",
        "Substitute back:\n",
        "$$\n",
        "\\mathbf{p} = \\mathbf{A} \\mathbf{z}\n",
        "= \\mathbf{A} (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{x}\n",
        "$$\n",
        "\n",
        "Define:\n",
        "$$\n",
        "\\mathbf{M} := \\mathbf{A} (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top\n",
        "\\Rightarrow \\mathrm{Proj}_{\\mathcal{X}} \\mathbf{x} = \\mathbf{M} \\mathbf{x}\n",
        "$$\n",
        "\n",
        "$\\textbf{Conclusion:}$\n",
        "When $\\mathcal{X}$ is a linear subspace (i.e., when $\\mathbf{b} = \\mathbf{0} $), the projection $\\mathrm{Proj}_{\\mathcal{X}} \\mathbf{x}$ is a linear map and can be expressed as $\\mathbf{M} \\mathbf{x}$ for some matrix $\\mathbf{M} \\in \\mathbb{R}^{n \\times n} $."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBA0nJKccVBf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define W (m x n) with full row rank\n",
        "W = np.array([[1, 2]])  # Shape: (1, 2)\n",
        "\n",
        "# Compute projection matrix onto null space of W\n",
        "Wt = W.T\n",
        "WWT_inv = np.linalg.inv(W @ Wt)\n",
        "M = np.eye(W.shape[1]) - Wt @ WWT_inv @ W\n",
        "\n",
        "print(\"Projection matrix M:\")\n",
        "print(M)\n",
        "\n",
        "# Project a vector x onto the null space of W\n",
        "x = np.array([2, 1])\n",
        "proj_x = M @ x\n",
        "\n",
        "print(\"Original vector x:\", x)\n",
        "print(\"Projection onto ker(W):\", proj_x)\n",
        "\n",
        "# Plot for 2D visualization\n",
        "origin = np.array([0, 0])\n",
        "plt.quiver(*origin, *x, color='blue', angles='xy', scale_units='xy', scale=1, label='x')\n",
        "plt.quiver(*origin, *proj_x, color='green', angles='xy', scale_units='xy', scale=1, label='Projection')\n",
        "\n",
        "# Line representing W x = 0 --> x + 2y = 0 --> y = -x/2\n",
        "x_vals = np.linspace(-4, 4, 100)\n",
        "y_vals = -x_vals / 2\n",
        "plt.plot(x_vals, y_vals, 'r--', label='ker(W)')\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.grid(True)\n",
        "plt.title('Projection onto the Null Space of W')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAvOerDkcVBf"
      },
      "source": [
        "### Exercise 7\n",
        "Show that for twice-differentiable convex functions $f$, we can write:\n",
        "$\n",
        "f(x + \\varepsilon) = f(x) + \\varepsilon f'(x) + \\frac{1}{2} \\varepsilon^2 f''(x + \\xi), \\quad \\text{for some } \\xi \\in [0, \\varepsilon]\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMU87o8TcVBf"
      },
      "source": [
        "$\\textbf{Solution.}$\n",
        "\n",
        "Let us define a new function $g(\\varepsilon)$ as follows:\n",
        "$$\n",
        "g(\\varepsilon) := f(x + \\varepsilon)\n",
        "$$\n",
        "Then:\n",
        "$$\n",
        "g(0) = f(x), \\quad g'(\\varepsilon) = f'(x + \\varepsilon), \\quad g''(\\varepsilon) = f''(x + \\varepsilon)\n",
        "$$\n",
        "\n",
        "Now, we apply the $\\textbf{second-order Taylor expansion}$ of $g(\\varepsilon)$ at $\\varepsilon = 0$ with the remainder in Lagrange form:\n",
        "$$\n",
        "g(\\varepsilon) = g(0) + \\varepsilon g'(0) + \\frac{1}{2} \\varepsilon^2 g''(\\xi) \\quad \\text{for some } \\xi \\in (0, \\varepsilon)\n",
        "$$\n",
        "\n",
        "Substituting back $g(\\varepsilon) = f(x + \\varepsilon)$, we get:\n",
        "$$\n",
        "f(x + \\varepsilon) = f(x) + \\varepsilon f'(x) + \\frac{1}{2} \\varepsilon^2 f''(x + \\xi) \\quad \\text{for some } \\xi \\in (0, \\varepsilon)\n",
        "$$\n",
        "\n",
        "$\\textbf{Conclusion:} $\n",
        "For any twice-differentiable function $f$, including all $C^2$ convex functions, the following holds:\n",
        "$$\n",
        "f(x + \\varepsilon) = f(x) + \\varepsilon f'(x) + \\frac{1}{2} \\varepsilon^2 f''(x + \\xi), \\quad \\text{for some } \\xi \\in [0, \\varepsilon]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgUg9BIJcVBf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define convex function f and its derivatives\n",
        "f = lambda x: np.exp(x)\n",
        "f_prime = lambda x: np.exp(x)\n",
        "f_double_prime = lambda x: np.exp(x)\n",
        "\n",
        "# Choose x and small epsilon\n",
        "x = 1.0\n",
        "eps = 0.5\n",
        "\n",
        "# Compute exact value\n",
        "f_x_eps = f(x + eps)\n",
        "\n",
        "# Compute Taylor approximation with unknown ξ ∈ [0, ε]\n",
        "# Try a range of ξ values to visualize the second-order approximation error\n",
        "xi_vals = np.linspace(0, eps, 100)\n",
        "approximations = f(x) + eps * f_prime(x) + 0.5 * eps**2 * f_double_prime(x + xi_vals)\n",
        "\n",
        "# Plot\n",
        "plt.plot(xi_vals, approximations, label='2nd order Taylor approx')\n",
        "plt.axhline(f_x_eps, color='r', linestyle='--', label='True f(x + ε)')\n",
        "plt.xlabel(r'$\\xi$ in $[0, \\epsilon]$')\n",
        "plt.title('Taylor Approximation vs. True Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VL4pcYEcVBf"
      },
      "source": [
        "### Exercise 8\n",
        "Given a convex set $\\mathcal{X}$ and two vectors $\\mathbf{x}$ and $\\mathbf{y}$, prove that projections never increase distances, i.e.,\n",
        "$$\n",
        "\\|\\mathbf{x} - \\mathbf{y}\\| \\geq \\|\\text{Proj}_{\\mathcal{X}}(\\mathbf{x}) - \\text{Proj}_{\\mathcal{X}}(\\mathbf{y})\\|.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpnh51T4cVBf"
      },
      "source": [
        "$\\textbf{Solution.}$\n",
        "\n",
        "Let $\\mathbf{p} := \\text{Proj}_{\\mathcal{X}}(\\mathbf{x})$ and $\\mathbf{q} := \\text{Proj}_{\\mathcal{X}}(\\mathbf{y})$ be the orthogonal projections of $\\mathbf{x}$ and $\\mathbf{y}$ onto the convex set $\\mathcal{X}$. A fundamental property of projections onto convex sets is the **firm non-expansiveness** property, which implies:\n",
        "\n",
        "$$\n",
        "\\langle \\mathbf{x} - \\mathbf{y}, \\mathbf{p} - \\mathbf{q} \\rangle \\geq \\|\\mathbf{p} - \\mathbf{q}\\|^2.\n",
        "$$\n",
        "\n",
        "Now, apply the Cauchy-Schwarz inequality to the left-hand side:\n",
        "\n",
        "$$\n",
        "\\langle \\mathbf{x} - \\mathbf{y}, \\mathbf{p} - \\mathbf{q} \\rangle \\leq \\|\\mathbf{x} - \\mathbf{y}\\| \\cdot \\|\\mathbf{p} - \\mathbf{q}\\|.\n",
        "$$\n",
        "\n",
        "Combining both inequalities:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{p} - \\mathbf{q}\\|^2 \\leq \\|\\mathbf{x} - \\mathbf{y}\\| \\cdot \\|\\mathbf{p} - \\mathbf{q}\\|.\n",
        "$$\n",
        "\n",
        "Divide both sides by $\\|\\mathbf{p} - \\mathbf{q}\\|$ (if non-zero), we obtain:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{p} - \\mathbf{q}\\| \\leq \\|\\mathbf{x} - \\mathbf{y}\\|.\n",
        "$$\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$$\n",
        "\\|\\text{Proj}_{\\mathcal{X}}(\\mathbf{x}) - \\text{Proj}_{\\mathcal{X}}(\\mathbf{y})\\| \\leq \\|\\mathbf{x} - \\mathbf{y}\\|.\n",
        "$$\n",
        "\n",
        "$\\textbf{Conclusion:}$ Projection onto a convex set is a non-expansive operation in Euclidean space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLzXCeWZcVBf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a convex set: unit ball (disk)\n",
        "def project_to_ball(x, radius=1.0):\n",
        "    norm = np.linalg.norm(x)\n",
        "    if norm <= radius:\n",
        "        return x\n",
        "    return x * (radius / norm)\n",
        "\n",
        "# Two vectors in R^2\n",
        "x = np.array([2.0, 1.5])\n",
        "y = np.array([1.0, -2.0])\n",
        "\n",
        "# Projections onto unit disk\n",
        "px = project_to_ball(x)\n",
        "py = project_to_ball(y)\n",
        "\n",
        "# Distances\n",
        "original_dist = np.linalg.norm(x - y)\n",
        "projected_dist = np.linalg.norm(px - py)\n",
        "\n",
        "print(\"||x - y|| =\", original_dist)\n",
        "print(\"||Proj(x) - Proj(y)|| =\", projected_dist)\n",
        "print(\"Non-expansiveness holds:\", projected_dist <= original_dist)\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots()\n",
        "circle = plt.Circle((0, 0), 1.0, color='lightblue', fill=True, label='Convex Set (Unit Ball)')\n",
        "ax.add_artist(circle)\n",
        "\n",
        "# Plot original and projected points\n",
        "ax.plot(*x, 'ro', label='x')\n",
        "ax.plot(*y, 'bo', label='y')\n",
        "ax.plot(*px, 'r^', label='Proj(x)')\n",
        "ax.plot(*py, 'b^', label='Proj(y)')\n",
        "\n",
        "# Connect vectors\n",
        "ax.plot([x[0], y[0]], [x[1], y[1]], 'k--', label='||x - y||')\n",
        "ax.plot([px[0], py[0]], [px[1], py[1]], 'g-', label='||Proj(x) - Proj(y)||')\n",
        "\n",
        "ax.set_aspect('equal')\n",
        "ax.set_xlim(-3, 3)\n",
        "ax.set_ylim(-3, 3)\n",
        "plt.title(\"Projection onto Convex Set Does Not Increase Distance\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}